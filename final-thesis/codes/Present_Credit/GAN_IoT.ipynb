{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "TrainN=20000, Val=250/250, TestB=200/200, TestI=10000/200\n",
      "[GANomaly] Epoch   1/60 | L_G=34.4831 | L_D=1.7746\n",
      "[GANomaly] Epoch   5/60 | L_G=22.1299 | L_D=1.4283\n",
      "[GANomaly] Epoch  10/60 | L_G=13.7023 | L_D=1.3943\n",
      "[GANomaly] Epoch  15/60 | L_G=10.7106 | L_D=1.4333\n",
      "[GANomaly] Epoch  20/60 | L_G=9.1880 | L_D=1.5825\n",
      "[GANomaly] Epoch  25/60 | L_G=8.7818 | L_D=1.4414\n",
      "[GANomaly] Epoch  30/60 | L_G=8.4787 | L_D=1.3326\n",
      "[GANomaly] Epoch  35/60 | L_G=7.7632 | L_D=1.6186\n",
      "[GANomaly] Epoch  40/60 | L_G=7.3751 | L_D=1.6302\n",
      "[GANomaly] Epoch  45/60 | L_G=6.8306 | L_D=1.6441\n",
      "[GANomaly] Epoch  50/60 | L_G=6.3804 | L_D=1.6933\n",
      "[GANomaly] Epoch  55/60 | L_G=6.3387 | L_D=1.6110\n",
      "[GANomaly] Epoch  60/60 | L_G=6.3350 | L_D=1.5395\n",
      "\n",
      "[VAL balanced] Best F1(Class 1)=0.876 at thr(norm)=0.027753\n",
      "\n",
      "===== TEST Balanced (200/200) @thr(norm)=0.027753 =====\n",
      "Confusion Matrix:\n",
      " [[192   8]\n",
      " [ 27 173]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8767    0.9600    0.9165       200\n",
      "           1     0.9558    0.8650    0.9081       200\n",
      "\n",
      "    accuracy                         0.9125       400\n",
      "   macro avg     0.9163    0.9125    0.9123       400\n",
      "weighted avg     0.9163    0.9125    0.9123       400\n",
      "\n",
      "ROC AUC (scores): 0.959925\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr(norm)=0.027753 =====\n",
      "Confusion Matrix:\n",
      " [[9540  460]\n",
      " [  27  173]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9972    0.9540    0.9751     10000\n",
      "           1     0.2733    0.8650    0.4154       200\n",
      "\n",
      "    accuracy                         0.9523     10200\n",
      "   macro avg     0.6352    0.9095    0.6952     10200\n",
      "weighted avg     0.9830    0.9523    0.9641     10200\n",
      "\n",
      "ROC AUC (scores): 0.9561105000000001\n",
      "\n",
      ">>> Quick sweep on percentile-based thresholds (norm-scale):\n",
      "\n",
      "-- Try thr(norm)=0.059302 (pctl=60) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr(norm)=0.059302 =====\n",
      "Confusion Matrix:\n",
      " [[197   3]\n",
      " [ 31 169]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8640    0.9850    0.9206       200\n",
      "           1     0.9826    0.8450    0.9086       200\n",
      "\n",
      "    accuracy                         0.9150       400\n",
      "   macro avg     0.9233    0.9150    0.9146       400\n",
      "weighted avg     0.9233    0.9150    0.9146       400\n",
      "\n",
      "ROC AUC (scores): 0.959925\n",
      "\n",
      "-- Try thr(norm)=0.059302 (pctl=60) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr(norm)=0.059302 =====\n",
      "Confusion Matrix:\n",
      " [[9834  166]\n",
      " [  31  169]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9969    0.9834    0.9901     10000\n",
      "           1     0.5045    0.8450    0.6318       200\n",
      "\n",
      "    accuracy                         0.9807     10200\n",
      "   macro avg     0.7507    0.9142    0.8109     10200\n",
      "weighted avg     0.9872    0.9807    0.9831     10200\n",
      "\n",
      "ROC AUC (scores): 0.9561105000000001\n",
      "\n",
      "-- Try thr(norm)=0.148863 (pctl=70) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr(norm)=0.148863 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [ 86 114]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6993    1.0000    0.8230       200\n",
      "           1     1.0000    0.5700    0.7261       200\n",
      "\n",
      "    accuracy                         0.7850       400\n",
      "   macro avg     0.8497    0.7850    0.7746       400\n",
      "weighted avg     0.8497    0.7850    0.7746       400\n",
      "\n",
      "ROC AUC (scores): 0.959925\n",
      "\n",
      "-- Try thr(norm)=0.148863 (pctl=70) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr(norm)=0.148863 =====\n",
      "Confusion Matrix:\n",
      " [[9974   26]\n",
      " [  86  114]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9915    0.9974    0.9944     10000\n",
      "           1     0.8143    0.5700    0.6706       200\n",
      "\n",
      "    accuracy                         0.9890     10200\n",
      "   macro avg     0.9029    0.7837    0.8325     10200\n",
      "weighted avg     0.9880    0.9890    0.9881     10200\n",
      "\n",
      "ROC AUC (scores): 0.9561105000000001\n",
      "\n",
      "-- Try thr(norm)=0.251050 (pctl=80) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr(norm)=0.251050 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [114  86]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6369    1.0000    0.7782       200\n",
      "           1     1.0000    0.4300    0.6014       200\n",
      "\n",
      "    accuracy                         0.7150       400\n",
      "   macro avg     0.8185    0.7150    0.6898       400\n",
      "weighted avg     0.8185    0.7150    0.6898       400\n",
      "\n",
      "ROC AUC (scores): 0.959925\n",
      "\n",
      "-- Try thr(norm)=0.251050 (pctl=80) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr(norm)=0.251050 =====\n",
      "Confusion Matrix:\n",
      " [[9991    9]\n",
      " [ 114   86]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9887    0.9991    0.9939     10000\n",
      "           1     0.9053    0.4300    0.5831       200\n",
      "\n",
      "    accuracy                         0.9879     10200\n",
      "   macro avg     0.9470    0.7146    0.7885     10200\n",
      "weighted avg     0.9871    0.9879    0.9858     10200\n",
      "\n",
      "ROC AUC (scores): 0.9561105000000001\n",
      "\n",
      "-- Try thr(norm)=0.330694 (pctl=85) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr(norm)=0.330694 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [131  69]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6042    1.0000    0.7533       200\n",
      "           1     1.0000    0.3450    0.5130       200\n",
      "\n",
      "    accuracy                         0.6725       400\n",
      "   macro avg     0.8021    0.6725    0.6332       400\n",
      "weighted avg     0.8021    0.6725    0.6332       400\n",
      "\n",
      "ROC AUC (scores): 0.959925\n",
      "\n",
      "-- Try thr(norm)=0.330694 (pctl=85) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr(norm)=0.330694 =====\n",
      "Confusion Matrix:\n",
      " [[9995    5]\n",
      " [ 131   69]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9871    0.9995    0.9932     10000\n",
      "           1     0.9324    0.3450    0.5036       200\n",
      "\n",
      "    accuracy                         0.9867     10200\n",
      "   macro avg     0.9597    0.6723    0.7484     10200\n",
      "weighted avg     0.9860    0.9867    0.9836     10200\n",
      "\n",
      "ROC AUC (scores): 0.9561105000000001\n",
      "\n",
      "-- Try thr(norm)=0.452567 (pctl=90) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr(norm)=0.452567 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [150  50]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5714    1.0000    0.7273       200\n",
      "           1     1.0000    0.2500    0.4000       200\n",
      "\n",
      "    accuracy                         0.6250       400\n",
      "   macro avg     0.7857    0.6250    0.5636       400\n",
      "weighted avg     0.7857    0.6250    0.5636       400\n",
      "\n",
      "ROC AUC (scores): 0.959925\n",
      "\n",
      "-- Try thr(norm)=0.452567 (pctl=90) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr(norm)=0.452567 =====\n",
      "Confusion Matrix:\n",
      " [[9997    3]\n",
      " [ 150   50]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9852    0.9997    0.9924     10000\n",
      "           1     0.9434    0.2500    0.3953       200\n",
      "\n",
      "    accuracy                         0.9850     10200\n",
      "   macro avg     0.9643    0.6249    0.6938     10200\n",
      "weighted avg     0.9844    0.9850    0.9807     10200\n",
      "\n",
      "ROC AUC (scores): 0.9561105000000001\n",
      "\n",
      "-- Try thr(norm)=0.693904 (pctl=92.5) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr(norm)=0.693904 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [157  43]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5602    1.0000    0.7181       200\n",
      "           1     1.0000    0.2150    0.3539       200\n",
      "\n",
      "    accuracy                         0.6075       400\n",
      "   macro avg     0.7801    0.6075    0.5360       400\n",
      "weighted avg     0.7801    0.6075    0.5360       400\n",
      "\n",
      "ROC AUC (scores): 0.959925\n",
      "\n",
      "-- Try thr(norm)=0.693904 (pctl=92.5) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr(norm)=0.693904 =====\n",
      "Confusion Matrix:\n",
      " [[9999    1]\n",
      " [ 157   43]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9845    0.9999    0.9922     10000\n",
      "           1     0.9773    0.2150    0.3525       200\n",
      "\n",
      "    accuracy                         0.9845     10200\n",
      "   macro avg     0.9809    0.6075    0.6723     10200\n",
      "weighted avg     0.9844    0.9845    0.9796     10200\n",
      "\n",
      "ROC AUC (scores): 0.9561105000000001\n",
      "\n",
      "-- Try thr(norm)=0.810804 (pctl=95) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr(norm)=0.810804 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [171  29]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5391    1.0000    0.7005       200\n",
      "           1     1.0000    0.1450    0.2533       200\n",
      "\n",
      "    accuracy                         0.5725       400\n",
      "   macro avg     0.7695    0.5725    0.4769       400\n",
      "weighted avg     0.7695    0.5725    0.4769       400\n",
      "\n",
      "ROC AUC (scores): 0.959925\n",
      "\n",
      "-- Try thr(norm)=0.810804 (pctl=95) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr(norm)=0.810804 =====\n",
      "Confusion Matrix:\n",
      " [[10000     0]\n",
      " [  171    29]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9832    1.0000    0.9915     10000\n",
      "           1     1.0000    0.1450    0.2533       200\n",
      "\n",
      "    accuracy                         0.9832     10200\n",
      "   macro avg     0.9916    0.5725    0.6224     10200\n",
      "weighted avg     0.9835    0.9832    0.9770     10200\n",
      "\n",
      "ROC AUC (scores): 0.9561105000000001\n",
      "\n",
      "-- Try thr(norm)=0.881369 (pctl=97.5) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr(norm)=0.881369 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [181  19]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5249    1.0000    0.6885       200\n",
      "           1     1.0000    0.0950    0.1735       200\n",
      "\n",
      "    accuracy                         0.5475       400\n",
      "   macro avg     0.7625    0.5475    0.4310       400\n",
      "weighted avg     0.7625    0.5475    0.4310       400\n",
      "\n",
      "ROC AUC (scores): 0.959925\n",
      "\n",
      "-- Try thr(norm)=0.881369 (pctl=97.5) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr(norm)=0.881369 =====\n",
      "Confusion Matrix:\n",
      " [[10000     0]\n",
      " [  181    19]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9822    1.0000    0.9910     10000\n",
      "           1     1.0000    0.0950    0.1735       200\n",
      "\n",
      "    accuracy                         0.9823     10200\n",
      "   macro avg     0.9911    0.5475    0.5823     10200\n",
      "weighted avg     0.9826    0.9823    0.9750     10200\n",
      "\n",
      "ROC AUC (scores): 0.9561105000000001\n",
      "\n",
      "-- Try thr(norm)=0.941418 (pctl=99) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr(norm)=0.941418 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [192   8]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5102    1.0000    0.6757       200\n",
      "           1     1.0000    0.0400    0.0769       200\n",
      "\n",
      "    accuracy                         0.5200       400\n",
      "   macro avg     0.7551    0.5200    0.3763       400\n",
      "weighted avg     0.7551    0.5200    0.3763       400\n",
      "\n",
      "ROC AUC (scores): 0.959925\n",
      "\n",
      "-- Try thr(norm)=0.941418 (pctl=99) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr(norm)=0.941418 =====\n",
      "Confusion Matrix:\n",
      " [[10000     0]\n",
      " [  192     8]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9812    1.0000    0.9905     10000\n",
      "           1     1.0000    0.0400    0.0769       200\n",
      "\n",
      "    accuracy                         0.9812     10200\n",
      "   macro avg     0.9906    0.5200    0.5337     10200\n",
      "weighted avg     0.9815    0.9812    0.9726     10200\n",
      "\n",
      "ROC AUC (scores): 0.9561105000000001\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# GANomaly for Tabular Anomaly Detection (creditcard.csv)\n",
    "# - Splits: TrainN≈20k normal, VAL 250/250, TEST Balanced 200/200, TEST Imbalanced 10k/200\n",
    "# - Scale: StandardScaler fit on train-normal\n",
    "# - Losses: \n",
    "#     * L_adv (feature matching)  -> ổn định GAN, không cần dùng BCE trực tiếp\n",
    "#     * L_con (reconstruction L1) -> tái tạo đầu vào\n",
    "#     * L_enc (latent consistency)-> ||E(x) - E(G(x))||_2\n",
    "# - Anomaly score = α * recon + β * latent (+ γ * feat)  (γ mặc định 0 cho tabular)\n",
    "# - Threshold: tối ưu F1 trên VAL; có sweep percentile (80..99)\n",
    "# ==========================================\n",
    "import os, random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    precision_recall_fscore_support, precision_recall_curve\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ------------------------------\n",
    "# 0) Reproducibility & device\n",
    "# ------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Load data\n",
    "# ------------------------------\n",
    "CSV = \"../creditcard.csv\"  # chỉnh đường dẫn nếu cần\n",
    "df = pd.read_csv(CSV)\n",
    "assert \"Class\" in df.columns\n",
    "X_df = df.drop(columns=[\"Class\"])\n",
    "y_all = df[\"Class\"].astype(int).to_numpy()\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Splits (như các mô hình trước)\n",
    "# ------------------------------\n",
    "normal_idx = np.where(y_all==0)[0]\n",
    "anom_idx   = np.where(y_all==1)[0]\n",
    "rng = np.random.default_rng(SEED); rng.shuffle(normal_idx); rng.shuffle(anom_idx)\n",
    "\n",
    "TR_N, VAL_N, VAL_A, TESTB_N, TESTB_A, TESTI_N = 20000, 250, 250, 200, 200, 10000\n",
    "assert len(anom_idx) >= (VAL_A + TESTB_A), \"Không đủ anomaly cho VAL/TESTB.\"\n",
    "\n",
    "max_train_normal = len(normal_idx) - (VAL_N + TESTB_N + TESTI_N)\n",
    "if max_train_normal < 1000:\n",
    "    TESTI_N = max(2000, len(normal_idx) - (VAL_N + TESTB_N + 1000))\n",
    "    max_train_normal = len(normal_idx) - (VAL_N + TESTB_N + TESTI_N)\n",
    "TRAIN_N = max(5000, min(TR_N, max_train_normal))\n",
    "\n",
    "ptr_n=0; ptr_a=0\n",
    "trn_n  = normal_idx[ptr_n:ptr_n+TRAIN_N]; ptr_n+=TRAIN_N\n",
    "val_n  = normal_idx[ptr_n:ptr_n+VAL_N];   ptr_n+=VAL_N\n",
    "tstb_n = normal_idx[ptr_n:ptr_n+TESTB_N]; ptr_n+=TESTB_N\n",
    "tsti_n = normal_idx[ptr_n:ptr_n+TESTI_N]; ptr_n+=TESTI_N\n",
    "\n",
    "val_a  = anom_idx[ptr_a:ptr_a+VAL_A];   ptr_a+=VAL_A\n",
    "tstb_a = anom_idx[ptr_a:ptr_a+TESTB_A]; ptr_a+=TESTB_A\n",
    "tsti_a = tstb_a\n",
    "\n",
    "def Xy(idxs): \n",
    "    return X_df.iloc[idxs].to_numpy().astype(np.float32), y_all[idxs]\n",
    "\n",
    "X_tr_n,_ = Xy(trn_n)\n",
    "X_val = np.vstack([X_df.iloc[val_n].to_numpy(), X_df.iloc[val_a].to_numpy()]).astype(np.float32)\n",
    "y_val = np.hstack([np.zeros(len(val_n),dtype=int), np.ones(len(val_a),dtype=int)])\n",
    "\n",
    "X_tstb = np.vstack([X_df.iloc[tstb_n].to_numpy(), X_df.iloc[tstb_a].to_numpy()]).astype(np.float32)\n",
    "y_tstb = np.hstack([np.zeros(len(tstb_n),dtype=int), np.ones(len(tstb_a),dtype=int)])\n",
    "\n",
    "X_tsti = np.vstack([X_df.iloc[tsti_n].to_numpy(), X_df.iloc[tsti_a].to_numpy()]).astype(np.float32)\n",
    "y_tsti = np.hstack([np.zeros(len(tsti_n),dtype=int), np.ones(len(tsti_a),dtype=int)])\n",
    "\n",
    "print(f\"TrainN={len(trn_n)}, Val={len(val_n)}/{len(val_a)}, TestB={len(tstb_n)}/{len(tstb_a)}, TestI={len(tsti_n)}/{len(tsti_a)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Scale\n",
    "# ------------------------------\n",
    "scaler = StandardScaler().fit(X_tr_n)\n",
    "def z(x): return scaler.transform(x).astype(np.float32)\n",
    "X_tr_n = z(X_tr_n); X_val = z(X_val); X_tstb = z(X_tstb); X_tsti = z(X_tsti)\n",
    "INPUT_DIM = X_tr_n.shape[1]\n",
    "\n",
    "# ------------------------------\n",
    "# 4) GANomaly network (MLP cho tabular)\n",
    "#     G: Enc(x)->z, Dec(z)->x', Enc'(x')->z'\n",
    "#     D: MLP trả vector đặc trưng (feature matching)\n",
    "# ------------------------------\n",
    "def mlp(in_dim, hidden, out_dim, last_act=None):\n",
    "    layers = [nn.Linear(in_dim, hidden), nn.LeakyReLU(0.2, inplace=True),\n",
    "              nn.Linear(hidden, hidden), nn.LeakyReLU(0.2, inplace=True),\n",
    "              nn.Linear(hidden, out_dim)]\n",
    "    if last_act == \"tanh\": layers.append(nn.Tanh())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, z_dim=32, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = mlp(in_dim, hidden, z_dim)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim=32, out_dim=INPUT_DIM, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = mlp(z_dim, hidden, out_dim)\n",
    "    def forward(self, z): return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=128, feat_dim=64):\n",
    "        super().__init__()\n",
    "        self.feat = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(hidden, feat_dim), nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        self.out = nn.Linear(feat_dim, 1)  # logit\n",
    "    def forward(self, x):\n",
    "        f = self.feat(x)\n",
    "        logit = self.out(f)\n",
    "        return logit, f\n",
    "\n",
    "class GANomaly(nn.Module):\n",
    "    def __init__(self, in_dim, z_dim=32, hidden=128):\n",
    "        super().__init__()\n",
    "        self.enc1 = Encoder(in_dim, z_dim, hidden)\n",
    "        self.dec  = Decoder(z_dim, in_dim, hidden)\n",
    "        self.enc2 = Encoder(in_dim, z_dim, hidden)  # encoder' cho x'\n",
    "        self.disc = Discriminator(in_dim, hidden, feat_dim=64)\n",
    "    def forward(self, x):\n",
    "        z  = self.enc1(x)\n",
    "        x_ = self.dec(z)\n",
    "        z_ = self.enc2(x_)\n",
    "        d_real, f_real = self.disc(x)\n",
    "        d_fake, f_fake = self.disc(x_)\n",
    "        return x_, z, z_, d_real, d_fake, f_real, f_fake\n",
    "\n",
    "model = GANomaly(INPUT_DIM, z_dim=32, hidden=128).to(device)\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Losses & Optimizers\n",
    "# ------------------------------\n",
    "LRg, LRd = 1e-4, 1e-4\n",
    "opt_G = optim.Adam(list(model.enc1.parameters()) + list(model.dec.parameters()) + list(model.enc2.parameters()), lr=LRg, betas=(0.5, 0.999))\n",
    "opt_D = optim.Adam(model.disc.parameters(), lr=LRd, betas=(0.5, 0.999))\n",
    "\n",
    "l1 = nn.L1Loss()\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# hệ số loss\n",
    "LAMBDA_CON = 50.0   # tái tạo (mạnh hơn cho tabular)\n",
    "LAMBDA_ENC =  1.0   # nhất quán latent\n",
    "LAMBDA_FM  = 10.0   # feature matching (ổn định GAN)\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Training (normal only) - fixed to avoid double-backward graph issue\n",
    "# ------------------------------\n",
    "BS, EPOCHS = 256, 60\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_tr_n)), batch_size=BS, shuffle=True)\n",
    "\n",
    "def set_requires_grad(module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    loss_g_tot = loss_d_tot = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for (xb,) in loader:\n",
    "        xb = xb.to(device)\n",
    "\n",
    "        # ==== (1) Update D ====\n",
    "        set_requires_grad(model.disc, True)   # bật grad cho D\n",
    "        opt_D.zero_grad()\n",
    "\n",
    "        # tạo x_hat để huấn luyện D, NGẮT gradient từ G\n",
    "        with torch.no_grad():\n",
    "            z_tmp  = model.enc1(xb)\n",
    "            x_hat_tmp = model.dec(z_tmp)\n",
    "\n",
    "        d_real, _ = model.disc(xb)\n",
    "        d_fake, _ = model.disc(x_hat_tmp.detach())\n",
    "\n",
    "        # hinge loss cho D\n",
    "        loss_d = torch.relu(1.0 - d_real).mean() + torch.relu(1.0 + d_fake).mean()\n",
    "        loss_d.backward()          # KHÔNG retain_graph\n",
    "        opt_D.step()\n",
    "\n",
    "        # ==== (2) Update G ====\n",
    "        set_requires_grad(model.disc, False)  # đóng băng D khi tối ưu G\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        # re-forward SAU khi D đã được update (graph mới hoàn toàn)\n",
    "        x_hat, z, z_hat, d_real2, d_fake2, f_real2, f_fake2 = model(xb)\n",
    "\n",
    "        # feature matching: không backprop vào D -> detach f_real2\n",
    "        loss_fm  = l1(f_real2.detach(), f_fake2)\n",
    "        loss_con = l1(x_hat, xb)                       # tái tạo L1 mean\n",
    "        loss_enc = torch.mean((z - z_hat) ** 2)        # latent consistency\n",
    "\n",
    "        loss_g = LAMBDA_FM * loss_fm + LAMBDA_CON * loss_con + LAMBDA_ENC * loss_enc\n",
    "        loss_g.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "        # thống kê\n",
    "        loss_g_tot += loss_g.item()\n",
    "        loss_d_tot += loss_d.item()\n",
    "        steps += 1\n",
    "\n",
    "    if ep == 1 or ep % 5 == 0:\n",
    "        print(f\"[GANomaly] Epoch {ep:3d}/{EPOCHS} | L_G={loss_g_tot/steps:.4f} | L_D={loss_d_tot/steps:.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7) Scoring (chuẩn hóa 0..1 theo VAL)\n",
    "#     score = α*recon + β*latent + γ*feat\n",
    "# ------------------------------\n",
    "ALPHA, BETA, GAMMA = 0.7, 0.3, 0.0  # tabular: chủ yếu recon + latent\n",
    "\n",
    "@torch.no_grad()\n",
    "def anomaly_score(x_np: np.ndarray):\n",
    "    model.eval()\n",
    "    xt = torch.from_numpy(x_np).to(device)\n",
    "    x_hat, z, z_hat, d_real, d_fake, f_real, f_fake = model(xt)\n",
    "    recon  = torch.mean(torch.abs(x_hat - xt), dim=1)         # L1 mean\n",
    "    latent = torch.mean((z - z_hat)**2, dim=1)                # L2 mean\n",
    "    feat   = torch.mean(torch.abs(f_real - f_fake), dim=1)    # option\n",
    "    s = ALPHA*recon + BETA*latent + GAMMA*feat\n",
    "    return s.detach().cpu().numpy().astype(np.float64), \\\n",
    "           recon.cpu().numpy(), latent.cpu().numpy(), feat.cpu().numpy()\n",
    "\n",
    "# Scores trên VAL để min–max\n",
    "val_s_raw, _, _, _ = anomaly_score(X_val)\n",
    "smin, smax = float(np.min(val_s_raw)), float(np.max(val_s_raw)); eps=1e-12\n",
    "def to_norm(s): return (s - smin) / (smax - smin + eps)\n",
    "\n",
    "val_s = to_norm(val_s_raw)\n",
    "\n",
    "# ------------------------------\n",
    "# 8) Chọn ngưỡng (trên thang 0..1) & Đánh giá\n",
    "# ------------------------------\n",
    "def best_f1_threshold(y_true, scores, percentiles=np.linspace(50,99.5,200)):\n",
    "    ths = np.percentile(scores, percentiles)\n",
    "    best_f1, best_thr = -1.0, None\n",
    "    for t in ths:\n",
    "        yhat = (scores >= t).astype(int)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_true, yhat, labels=[0,1], average=None, zero_division=0)\n",
    "        if f1[1] > best_f1: best_f1, best_thr = float(f1[1]), float(t)\n",
    "    return best_thr, best_f1\n",
    "\n",
    "thr_norm, f1v = best_f1_threshold(y_val, val_s)\n",
    "print(f\"\\n[VAL balanced] Best F1(Class 1)={f1v:.3f} at thr(norm)={thr_norm:.6f}\")\n",
    "\n",
    "def evaluate(name, X, y, thr):\n",
    "    s_raw,_,_,_ = anomaly_score(X)\n",
    "    s = to_norm(s_raw)  # dùng thang chuẩn hóa 0..1\n",
    "    yhat = (s >= thr).astype(int)\n",
    "    print(f\"\\n===== {name} @thr(norm)={thr:.6f} =====\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y, yhat))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y, yhat, digits=4))\n",
    "    print(\"ROC AUC (scores):\", roc_auc_score(y, s))\n",
    "\n",
    "evaluate(\"TEST Balanced (200/200)\", X_tstb, y_tstb, thr_norm)\n",
    "evaluate(\"TEST Imbalanced (10000/200)\", X_tsti, y_tsti, thr_norm)\n",
    "\n",
    "# Sweep vài percentile để quan sát trade-off\n",
    "print(\"\\n>>> Quick sweep on percentile-based thresholds (norm-scale):\")\n",
    "for pctl in [60,70,80,85,90,92.5,95,97.5,99]:\n",
    "    t = float(np.percentile(val_s, pctl))\n",
    "    print(f\"\\n-- Try thr(norm)={t:.6f} (pctl={pctl}) on TEST Balanced --\")\n",
    "    evaluate(\"TEST Balanced (200/200)\", X_tstb, y_tstb, t)\n",
    "    print(f\"\\n-- Try thr(norm)={t:.6f} (pctl={pctl}) on TEST Imbalanced --\")\n",
    "    evaluate(\"TEST Imbalanced (10000/200)\", X_tsti, y_tsti, t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
