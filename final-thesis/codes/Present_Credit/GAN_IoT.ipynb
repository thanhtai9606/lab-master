{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "TrainN=20000, Val=250/250, TestB=200/200, TestI=10000/200\n",
      "[GAN] Epoch   1/60 | D=1.20100 | G=0.70563\n",
      "[GAN] Epoch   5/60 | D=1.04195 | G=1.28119\n",
      "[GAN] Epoch  10/60 | D=1.12159 | G=1.21620\n",
      "[GAN] Epoch  15/60 | D=1.20607 | G=1.24549\n",
      "[GAN] Epoch  20/60 | D=1.09469 | G=1.27483\n",
      "[GAN] Epoch  25/60 | D=1.15619 | G=1.30272\n",
      "[GAN] Epoch  30/60 | D=1.10116 | G=1.13824\n",
      "[GAN] Epoch  35/60 | D=1.14603 | G=1.32037\n",
      "[GAN] Epoch  40/60 | D=1.07364 | G=1.40558\n",
      "[GAN] Epoch  45/60 | D=1.02368 | G=1.33516\n",
      "[GAN] Epoch  50/60 | D=1.01686 | G=1.34203\n",
      "[GAN] Epoch  55/60 | D=0.95335 | G=1.42906\n",
      "[GAN] Epoch  60/60 | D=0.90034 | G=1.65828\n",
      "\n",
      "[VAL balanced] Best F1(Class 1)=0.748 at threshold=0.692722\n",
      "\n",
      "===== TEST Balanced (200/200) @thr=0.692722 =====\n",
      "Confusion Matrix:\n",
      " [[167  33]\n",
      " [ 75 125]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6901    0.8350    0.7557       200\n",
      "           1     0.7911    0.6250    0.6983       200\n",
      "\n",
      "    accuracy                         0.7300       400\n",
      "   macro avg     0.7406    0.7300    0.7270       400\n",
      "weighted avg     0.7406    0.7300    0.7270       400\n",
      "\n",
      "ROC AUC (scores): 0.7413000000000001\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr=0.692722 =====\n",
      "Confusion Matrix:\n",
      " [[9035  965]\n",
      " [  75  125]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9918    0.9035    0.9456     10000\n",
      "           1     0.1147    0.6250    0.1938       200\n",
      "\n",
      "    accuracy                         0.8980     10200\n",
      "   macro avg     0.5532    0.7642    0.5697     10200\n",
      "weighted avg     0.9746    0.8980    0.9308     10200\n",
      "\n",
      "ROC AUC (scores): 0.7513724999999998\n",
      "\n",
      ">>> Quick sweep on percentile-based thresholds (from VAL):\n",
      "\n",
      "-- Try thr=0.980733 (pctl=80) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr=0.980733 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [116  84]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6329    1.0000    0.7752       200\n",
      "           1     1.0000    0.4200    0.5915       200\n",
      "\n",
      "    accuracy                         0.7100       400\n",
      "   macro avg     0.8165    0.7100    0.6834       400\n",
      "weighted avg     0.8165    0.7100    0.6834       400\n",
      "\n",
      "ROC AUC (scores): 0.7413000000000001\n",
      "\n",
      "-- Try thr=0.980733 (pctl=80) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr=0.980733 =====\n",
      "Confusion Matrix:\n",
      " [[10000     0]\n",
      " [  116    84]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9885    1.0000    0.9942     10000\n",
      "           1     1.0000    0.4200    0.5915       200\n",
      "\n",
      "    accuracy                         0.9886     10200\n",
      "   macro avg     0.9943    0.7100    0.7929     10200\n",
      "weighted avg     0.9888    0.9886    0.9863     10200\n",
      "\n",
      "ROC AUC (scores): 0.7513724999999998\n",
      "\n",
      "-- Try thr=0.999275 (pctl=90) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr=0.999275 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [151  49]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5698    1.0000    0.7260       200\n",
      "           1     1.0000    0.2450    0.3936       200\n",
      "\n",
      "    accuracy                         0.6225       400\n",
      "   macro avg     0.7849    0.6225    0.5598       400\n",
      "weighted avg     0.7849    0.6225    0.5598       400\n",
      "\n",
      "ROC AUC (scores): 0.7413000000000001\n",
      "\n",
      "-- Try thr=0.999275 (pctl=90) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr=0.999275 =====\n",
      "Confusion Matrix:\n",
      " [[10000     0]\n",
      " [  151    49]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9851    1.0000    0.9925     10000\n",
      "           1     1.0000    0.2450    0.3936       200\n",
      "\n",
      "    accuracy                         0.9852     10200\n",
      "   macro avg     0.9926    0.6225    0.6930     10200\n",
      "weighted avg     0.9854    0.9852    0.9808     10200\n",
      "\n",
      "ROC AUC (scores): 0.7513724999999998\n",
      "\n",
      "-- Try thr=0.999791 (pctl=92.5) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr=0.999791 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [167  33]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5450    1.0000    0.7055       200\n",
      "           1     1.0000    0.1650    0.2833       200\n",
      "\n",
      "    accuracy                         0.5825       400\n",
      "   macro avg     0.7725    0.5825    0.4944       400\n",
      "weighted avg     0.7725    0.5825    0.4944       400\n",
      "\n",
      "ROC AUC (scores): 0.7413000000000001\n",
      "\n",
      "-- Try thr=0.999791 (pctl=92.5) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr=0.999791 =====\n",
      "Confusion Matrix:\n",
      " [[10000     0]\n",
      " [  167    33]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9836    1.0000    0.9917     10000\n",
      "           1     1.0000    0.1650    0.2833       200\n",
      "\n",
      "    accuracy                         0.9836     10200\n",
      "   macro avg     0.9918    0.5825    0.6375     10200\n",
      "weighted avg     0.9839    0.9836    0.9778     10200\n",
      "\n",
      "ROC AUC (scores): 0.7513724999999998\n",
      "\n",
      "-- Try thr=0.999966 (pctl=95) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr=0.999966 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [177  23]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5305    1.0000    0.6932       200\n",
      "           1     1.0000    0.1150    0.2063       200\n",
      "\n",
      "    accuracy                         0.5575       400\n",
      "   macro avg     0.7653    0.5575    0.4498       400\n",
      "weighted avg     0.7653    0.5575    0.4498       400\n",
      "\n",
      "ROC AUC (scores): 0.7413000000000001\n",
      "\n",
      "-- Try thr=0.999966 (pctl=95) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr=0.999966 =====\n",
      "Confusion Matrix:\n",
      " [[10000     0]\n",
      " [  177    23]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9826    1.0000    0.9912     10000\n",
      "           1     1.0000    0.1150    0.2063       200\n",
      "\n",
      "    accuracy                         0.9826     10200\n",
      "   macro avg     0.9913    0.5575    0.5988     10200\n",
      "weighted avg     0.9829    0.9826    0.9758     10200\n",
      "\n",
      "ROC AUC (scores): 0.7513724999999998\n",
      "\n",
      "-- Try thr=0.999995 (pctl=97.5) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr=0.999995 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [191   9]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5115    1.0000    0.6768       200\n",
      "           1     1.0000    0.0450    0.0861       200\n",
      "\n",
      "    accuracy                         0.5225       400\n",
      "   macro avg     0.7558    0.5225    0.3815       400\n",
      "weighted avg     0.7558    0.5225    0.3815       400\n",
      "\n",
      "ROC AUC (scores): 0.7413000000000001\n",
      "\n",
      "-- Try thr=0.999995 (pctl=97.5) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr=0.999995 =====\n",
      "Confusion Matrix:\n",
      " [[10000     0]\n",
      " [  191     9]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9813    1.0000    0.9905     10000\n",
      "           1     1.0000    0.0450    0.0861       200\n",
      "\n",
      "    accuracy                         0.9813     10200\n",
      "   macro avg     0.9906    0.5225    0.5383     10200\n",
      "weighted avg     0.9816    0.9813    0.9728     10200\n",
      "\n",
      "ROC AUC (scores): 0.7513724999999998\n",
      "\n",
      "-- Try thr=0.999998 (pctl=99) on TEST Balanced --\n",
      "\n",
      "===== TEST Balanced (200/200) @thr=0.999998 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [194   6]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5076    1.0000    0.6734       200\n",
      "           1     1.0000    0.0300    0.0583       200\n",
      "\n",
      "    accuracy                         0.5150       400\n",
      "   macro avg     0.7538    0.5150    0.3658       400\n",
      "weighted avg     0.7538    0.5150    0.3658       400\n",
      "\n",
      "ROC AUC (scores): 0.7413000000000001\n",
      "\n",
      "-- Try thr=0.999998 (pctl=99) on TEST Imbalanced --\n",
      "\n",
      "===== TEST Imbalanced (10000/200) @thr=0.999998 =====\n",
      "Confusion Matrix:\n",
      " [[10000     0]\n",
      " [  194     6]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9810    1.0000    0.9904     10000\n",
      "           1     1.0000    0.0300    0.0583       200\n",
      "\n",
      "    accuracy                         0.9810     10200\n",
      "   macro avg     0.9905    0.5150    0.5243     10200\n",
      "weighted avg     0.9813    0.9810    0.9721     10200\n",
      "\n",
      "ROC AUC (scores): 0.7513724999999998\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# GAN anomaly detection (creditcard.csv) with manual/F1/Precision threshold\n",
    "# - Stabilization: BCEWithLogitsLoss, label smoothing, input noise, Adam betas(0.5,0.999)\n",
    "# - Splits: TrainN=20k normal, VAL 250/250, TEST Balanced 200/200, TEST Imbalanced 10k/200\n",
    "# ==========================================\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    precision_recall_fscore_support, precision_recall_curve\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ------------------------------\n",
    "# 0) Seeds & device\n",
    "# ------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Load\n",
    "# ------------------------------\n",
    "csv_path = \"../creditcard.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "assert \"Class\" in df.columns\n",
    "X_df = df.drop(columns=[\"Class\"])\n",
    "y = df[\"Class\"].astype(int).to_numpy()\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Splits (no overlap)\n",
    "# ------------------------------\n",
    "normal_idx = np.where(y==0)[0]; anom_idx = np.where(y==1)[0]\n",
    "rng = np.random.default_rng(SEED); rng.shuffle(normal_idx); rng.shuffle(anom_idx)\n",
    "\n",
    "TR_N, VAL_N, VAL_A, TESTB_N, TESTB_A, TESTI_N = 20000, 250, 250, 200, 200, 10000\n",
    "assert len(anom_idx) >= (VAL_A + TESTB_A), \"Không đủ anomaly.\"\n",
    "\n",
    "max_train_normal = len(normal_idx) - (VAL_N + TESTB_N + TESTI_N)\n",
    "if max_train_normal < 1000:\n",
    "    TESTI_N = max(2000, len(normal_idx) - (VAL_N + TESTB_N + 1000))\n",
    "    max_train_normal = len(normal_idx) - (VAL_N + TESTB_N + TESTI_N)\n",
    "TRAIN_N = max(5000, min(TR_N, max_train_normal))\n",
    "\n",
    "ptr_n=0; ptr_a=0\n",
    "trn_n = normal_idx[ptr_n:ptr_n+TRAIN_N]; ptr_n+=TRAIN_N\n",
    "val_n = normal_idx[ptr_n:ptr_n+VAL_N];   ptr_n+=VAL_N\n",
    "tstb_n= normal_idx[ptr_n:ptr_n+TESTB_N]; ptr_n+=TESTB_N\n",
    "tsti_n= normal_idx[ptr_n:ptr_n+TESTI_N]; ptr_n+=TESTI_N\n",
    "\n",
    "val_a = anom_idx[ptr_a:ptr_a+VAL_A]; ptr_a+=VAL_A\n",
    "tstb_a= anom_idx[ptr_a:ptr_a+TESTB_A]; ptr_a+=TESTB_A\n",
    "tsti_a= tstb_a\n",
    "\n",
    "def take(idxs): return X_df.iloc[idxs].to_numpy().astype(np.float32), y[idxs]\n",
    "X_tr_n, _ = take(trn_n)\n",
    "\n",
    "X_val = np.vstack([X_df.iloc[val_n].to_numpy(), X_df.iloc[val_a].to_numpy()]).astype(np.float32)\n",
    "y_val = np.hstack([np.zeros(len(val_n), dtype=int), np.ones(len(val_a), dtype=int)])\n",
    "\n",
    "X_tstb = np.vstack([X_df.iloc[tstb_n].to_numpy(), X_df.iloc[tstb_a].to_numpy()]).astype(np.float32)\n",
    "y_tstb = np.hstack([np.zeros(len(tstb_n), dtype=int), np.ones(len(tstb_a), dtype=int)])\n",
    "\n",
    "X_tsti = np.vstack([X_df.iloc[tsti_n].to_numpy(), X_df.iloc[tsti_a].to_numpy()]).astype(np.float32)\n",
    "y_tsti = np.hstack([np.zeros(len(tsti_n), dtype=int), np.ones(len(tsti_a), dtype=int)])\n",
    "\n",
    "print(f\"TrainN={len(trn_n)}, Val={len(val_n)}/{len(val_a)}, TestB={len(tstb_n)}/{len(tstb_a)}, TestI={len(tsti_n)}/{len(tsti_a)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Scale (fit on train normal)\n",
    "# ------------------------------\n",
    "scaler = StandardScaler().fit(X_tr_n)\n",
    "def z(x): return scaler.transform(x).astype(np.float32)\n",
    "X_tr_n = z(X_tr_n); X_val = z(X_val); X_tstb = z(X_tstb); X_tsti = z(X_tsti)\n",
    "\n",
    "# ------------------------------\n",
    "# 4) GAN\n",
    "# ------------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 256), nn.ReLU(),\n",
    "            nn.Linear(256, out_dim)\n",
    "        )\n",
    "    def forward(self, z): return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # logits\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "input_dim = X_tr_n.shape[1]; latent_dim = 32\n",
    "G = Generator(latent_dim, input_dim).to(device)\n",
    "D = Discriminator(input_dim).to(device)\n",
    "\n",
    "bce_logits = nn.BCEWithLogitsLoss()\n",
    "opt_g = optim.Adam(G.parameters(), lr=2e-4, betas=(0.5,0.999))\n",
    "opt_d = optim.Adam(D.parameters(), lr=2e-4, betas=(0.5,0.999))\n",
    "\n",
    "bs, epochs = 256, 60\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_tr_n)), batch_size=bs, shuffle=True, drop_last=False)\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    G.train(); D.train()\n",
    "    dsum=gsum=0.0; steps=0\n",
    "    for (xb,) in loader:\n",
    "        xb = xb.to(device)\n",
    "        bsz = xb.size(0)\n",
    "        real_noisy = xb + 0.01 * torch.randn_like(xb)\n",
    "\n",
    "        # Train D\n",
    "        opt_d.zero_grad()\n",
    "        z = torch.randn(bsz, latent_dim, device=device)\n",
    "        fake = G(z).detach()\n",
    "        real_lbl = torch.full((bsz,1), 0.9, device=device)\n",
    "        fake_lbl = torch.zeros(bsz,1, device=device)\n",
    "        d_real = D(real_noisy)\n",
    "        d_fake = D(fake)\n",
    "        d_loss = bce_logits(d_real, real_lbl) + bce_logits(d_fake, fake_lbl)\n",
    "        d_loss.backward(); opt_d.step()\n",
    "\n",
    "        # Train G\n",
    "        opt_g.zero_grad()\n",
    "        z = torch.randn(bsz, latent_dim, device=device)\n",
    "        gen = G(z)\n",
    "        g_loss = bce_logits(D(gen), torch.ones(bsz,1,device=device))\n",
    "        g_loss.backward(); opt_g.step()\n",
    "\n",
    "        dsum += d_loss.item(); gsum += g_loss.item(); steps += 1\n",
    "    if ep==1 or ep%5==0:\n",
    "        print(f\"[GAN] Epoch {ep:3d}/{epochs} | D={dsum/steps:.5f} | G={gsum/steps:.5f}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def disc_score(x_np: np.ndarray):\n",
    "    D.eval()\n",
    "    xt = torch.from_numpy(x_np).to(device)\n",
    "    logits = D(xt).cpu().numpy().reshape(-1)\n",
    "    probs  = 1.0/(1.0+np.exp(-logits))\n",
    "    return 1.0 - probs  # higher => more anomalous\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Threshold selection\n",
    "# ------------------------------\n",
    "MODE = \"f1\"           # \"manual\" | \"f1\" | \"p_at\"\n",
    "THR_MANUAL = 0.60     # dùng khi MODE=\"manual\"\n",
    "TARGET_P   = 0.60     # dùng khi MODE=\"p_at\"\n",
    "\n",
    "val_scores = disc_score(X_val)\n",
    "\n",
    "def best_f1_threshold(y_true, scores, percentiles=np.linspace(50, 99.5, 200)):\n",
    "    ths = np.percentile(scores, percentiles)\n",
    "    best_f1, best_thr = -1.0, None\n",
    "    for t in ths:\n",
    "        yhat = (scores >= t).astype(int)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, yhat, labels=[0,1], average=None, zero_division=0\n",
    "        )\n",
    "        if f1[1] > best_f1:\n",
    "            best_f1, best_thr = float(f1[1]), float(t)\n",
    "    return best_thr, best_f1\n",
    "\n",
    "def threshold_for_precision(y_true, scores, target_p=0.60):\n",
    "    p, r, thr = precision_recall_curve(y_true, scores)\n",
    "    idx = np.where(p[:-1] >= target_p)[0]\n",
    "    if len(idx) == 0:\n",
    "        # fallback: dùng percentile 95 nếu không đạt target precision\n",
    "        return float(np.percentile(scores, 95)), float(p[1] if len(p)>1 else 0.0), float(r[1] if len(r)>1 else 0.0)\n",
    "    i = idx[0]\n",
    "    return float(thr[i]), float(p[i]), float(r[i])\n",
    "\n",
    "if MODE == \"manual\":\n",
    "    thr = float(THR_MANUAL)\n",
    "    pct = (val_scores <= thr).mean() * 100.0\n",
    "    print(f\"\\n[VAL] Manual threshold selected: thr={thr:.6f} (~percentile {pct:.2f}%)\")\n",
    "elif MODE == \"f1\":\n",
    "    thr, f1v = best_f1_threshold(y_val, val_scores)\n",
    "    print(f\"\\n[VAL balanced] Best F1(Class 1)={f1v:.3f} at threshold={thr:.6f}\")\n",
    "else:  # \"p_at\"\n",
    "    thr, p_at, r_at = threshold_for_precision(y_val, val_scores, TARGET_P)\n",
    "    print(f\"\\n[VAL balanced] Threshold for Precision≥{TARGET_P:.2f}: thr={thr:.6f} (P={p_at:.3f}, R={r_at:.3f})\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Evaluate\n",
    "# ------------------------------\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "def evaluate(name, X, y, thr):\n",
    "    s = disc_score(X); yhat = (s >= thr).astype(int)\n",
    "    print(f\"\\n===== {name} @thr={thr:.6f} =====\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y, yhat))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y, yhat, digits=4))\n",
    "    print(\"ROC AUC (scores):\", roc_auc_score(y, s))\n",
    "\n",
    "evaluate(\"TEST Balanced (200/200)\",     X_tstb, y_tstb, thr)\n",
    "evaluate(\"TEST Imbalanced (10000/200)\", X_tsti, y_tsti, thr)\n",
    "\n",
    "# ------------------------------\n",
    "# 7) Quick sweep vài percentile để quan sát\n",
    "# ------------------------------\n",
    "cands = [50, 60, 70,80, 90, 92.5, 95, 97.5, 99]\n",
    "print(\"\\n>>> Quick sweep on percentile-based thresholds (from VAL):\")\n",
    "for p in cands:\n",
    "    t = float(np.percentile(val_scores, p))\n",
    "    print(f\"\\n-- Try thr={t:.6f} (pctl={p}) on TEST Balanced --\")\n",
    "    evaluate(\"TEST Balanced (200/200)\", X_tstb, y_tstb, t)\n",
    "    print(f\"\\n-- Try thr={t:.6f} (pctl={p}) on TEST Imbalanced --\")\n",
    "    evaluate(\"TEST Imbalanced (10000/200)\", X_tsti, y_tsti, t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
