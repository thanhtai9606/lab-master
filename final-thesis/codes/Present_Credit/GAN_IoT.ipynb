{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "TrainN=20000, Val=250/250, TestB=200/200, TestI=10000/200\n",
      "[GANomaly] Epoch   1/60 | L_G=34.4831 | L_D=1.7746\n",
      "[GANomaly] Epoch   5/60 | L_G=22.1299 | L_D=1.4283\n",
      "[GANomaly] Epoch  10/60 | L_G=13.7023 | L_D=1.3943\n",
      "[GANomaly] Epoch  15/60 | L_G=10.7106 | L_D=1.4333\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# GANomaly for Tabular Anomaly Detection (creditcard.csv)\n",
    "# - Splits: TrainN≈20k normal, VAL 250/250, TEST Balanced 200/200, TEST Imbalanced 10k/200\n",
    "# - Scale: StandardScaler fit on train-normal\n",
    "# - Losses: \n",
    "#     * L_adv (feature matching)  -> ổn định GAN, không cần dùng BCE trực tiếp\n",
    "#     * L_con (reconstruction L1) -> tái tạo đầu vào\n",
    "#     * L_enc (latent consistency)-> ||E(x) - E(G(x))||_2\n",
    "# - Anomaly score = α * recon + β * latent (+ γ * feat)  (γ mặc định 0 cho tabular)\n",
    "# - Threshold: tối ưu F1 trên VAL; có sweep percentile (80..99)\n",
    "# ==========================================\n",
    "import os, random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    precision_recall_fscore_support, precision_recall_curve\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ------------------------------\n",
    "# 0) Reproducibility & device\n",
    "# ------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Load data\n",
    "# ------------------------------\n",
    "CSV = \"../creditcard.csv\"  # chỉnh đường dẫn nếu cần\n",
    "df = pd.read_csv(CSV)\n",
    "assert \"Class\" in df.columns\n",
    "X_df = df.drop(columns=[\"Class\"])\n",
    "y_all = df[\"Class\"].astype(int).to_numpy()\n",
    "\n",
    "# ------------------------------\n",
    "# 2) Splits (như các mô hình trước)\n",
    "# ------------------------------\n",
    "normal_idx = np.where(y_all==0)[0]\n",
    "anom_idx   = np.where(y_all==1)[0]\n",
    "rng = np.random.default_rng(SEED); rng.shuffle(normal_idx); rng.shuffle(anom_idx)\n",
    "\n",
    "TR_N, VAL_N, VAL_A, TESTB_N, TESTB_A, TESTI_N = 20000, 250, 250, 200, 200, 10000\n",
    "assert len(anom_idx) >= (VAL_A + TESTB_A), \"Không đủ anomaly cho VAL/TESTB.\"\n",
    "\n",
    "max_train_normal = len(normal_idx) - (VAL_N + TESTB_N + TESTI_N)\n",
    "if max_train_normal < 1000:\n",
    "    TESTI_N = max(2000, len(normal_idx) - (VAL_N + TESTB_N + 1000))\n",
    "    max_train_normal = len(normal_idx) - (VAL_N + TESTB_N + TESTI_N)\n",
    "TRAIN_N = max(5000, min(TR_N, max_train_normal))\n",
    "\n",
    "ptr_n=0; ptr_a=0\n",
    "trn_n  = normal_idx[ptr_n:ptr_n+TRAIN_N]; ptr_n+=TRAIN_N\n",
    "val_n  = normal_idx[ptr_n:ptr_n+VAL_N];   ptr_n+=VAL_N\n",
    "tstb_n = normal_idx[ptr_n:ptr_n+TESTB_N]; ptr_n+=TESTB_N\n",
    "tsti_n = normal_idx[ptr_n:ptr_n+TESTI_N]; ptr_n+=TESTI_N\n",
    "\n",
    "val_a  = anom_idx[ptr_a:ptr_a+VAL_A];   ptr_a+=VAL_A\n",
    "tstb_a = anom_idx[ptr_a:ptr_a+TESTB_A]; ptr_a+=TESTB_A\n",
    "tsti_a = tstb_a\n",
    "\n",
    "def Xy(idxs): \n",
    "    return X_df.iloc[idxs].to_numpy().astype(np.float32), y_all[idxs]\n",
    "\n",
    "X_tr_n,_ = Xy(trn_n)\n",
    "X_val = np.vstack([X_df.iloc[val_n].to_numpy(), X_df.iloc[val_a].to_numpy()]).astype(np.float32)\n",
    "y_val = np.hstack([np.zeros(len(val_n),dtype=int), np.ones(len(val_a),dtype=int)])\n",
    "\n",
    "X_tstb = np.vstack([X_df.iloc[tstb_n].to_numpy(), X_df.iloc[tstb_a].to_numpy()]).astype(np.float32)\n",
    "y_tstb = np.hstack([np.zeros(len(tstb_n),dtype=int), np.ones(len(tstb_a),dtype=int)])\n",
    "\n",
    "X_tsti = np.vstack([X_df.iloc[tsti_n].to_numpy(), X_df.iloc[tsti_a].to_numpy()]).astype(np.float32)\n",
    "y_tsti = np.hstack([np.zeros(len(tsti_n),dtype=int), np.ones(len(tsti_a),dtype=int)])\n",
    "\n",
    "print(f\"TrainN={len(trn_n)}, Val={len(val_n)}/{len(val_a)}, TestB={len(tstb_n)}/{len(tstb_a)}, TestI={len(tsti_n)}/{len(tsti_a)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3) Scale\n",
    "# ------------------------------\n",
    "scaler = StandardScaler().fit(X_tr_n)\n",
    "def z(x): return scaler.transform(x).astype(np.float32)\n",
    "X_tr_n = z(X_tr_n); X_val = z(X_val); X_tstb = z(X_tstb); X_tsti = z(X_tsti)\n",
    "INPUT_DIM = X_tr_n.shape[1]\n",
    "\n",
    "# ------------------------------\n",
    "# 4) GANomaly network (MLP cho tabular)\n",
    "#     G: Enc(x)->z, Dec(z)->x', Enc'(x')->z'\n",
    "#     D: MLP trả vector đặc trưng (feature matching)\n",
    "# ------------------------------\n",
    "def mlp(in_dim, hidden, out_dim, last_act=None):\n",
    "    layers = [nn.Linear(in_dim, hidden), nn.LeakyReLU(0.2, inplace=True),\n",
    "              nn.Linear(hidden, hidden), nn.LeakyReLU(0.2, inplace=True),\n",
    "              nn.Linear(hidden, out_dim)]\n",
    "    if last_act == \"tanh\": layers.append(nn.Tanh())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, z_dim=32, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = mlp(in_dim, hidden, z_dim)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim=32, out_dim=INPUT_DIM, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = mlp(z_dim, hidden, out_dim)\n",
    "    def forward(self, z): return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=128, feat_dim=64):\n",
    "        super().__init__()\n",
    "        self.feat = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(hidden, feat_dim), nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        self.out = nn.Linear(feat_dim, 1)  # logit\n",
    "    def forward(self, x):\n",
    "        f = self.feat(x)\n",
    "        logit = self.out(f)\n",
    "        return logit, f\n",
    "\n",
    "class GANomaly(nn.Module):\n",
    "    def __init__(self, in_dim, z_dim=32, hidden=128):\n",
    "        super().__init__()\n",
    "        self.enc1 = Encoder(in_dim, z_dim, hidden)\n",
    "        self.dec  = Decoder(z_dim, in_dim, hidden)\n",
    "        self.enc2 = Encoder(in_dim, z_dim, hidden)  # encoder' cho x'\n",
    "        self.disc = Discriminator(in_dim, hidden, feat_dim=64)\n",
    "    def forward(self, x):\n",
    "        z  = self.enc1(x)\n",
    "        x_ = self.dec(z)\n",
    "        z_ = self.enc2(x_)\n",
    "        d_real, f_real = self.disc(x)\n",
    "        d_fake, f_fake = self.disc(x_)\n",
    "        return x_, z, z_, d_real, d_fake, f_real, f_fake\n",
    "\n",
    "model = GANomaly(INPUT_DIM, z_dim=32, hidden=128).to(device)\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Losses & Optimizers\n",
    "# ------------------------------\n",
    "LRg, LRd = 1e-4, 1e-4\n",
    "opt_G = optim.Adam(list(model.enc1.parameters()) + list(model.dec.parameters()) + list(model.enc2.parameters()), lr=LRg, betas=(0.5, 0.999))\n",
    "opt_D = optim.Adam(model.disc.parameters(), lr=LRd, betas=(0.5, 0.999))\n",
    "\n",
    "l1 = nn.L1Loss()\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# hệ số loss\n",
    "LAMBDA_CON = 50.0   # tái tạo (mạnh hơn cho tabular)\n",
    "LAMBDA_ENC =  1.0   # nhất quán latent\n",
    "LAMBDA_FM  = 10.0   # feature matching (ổn định GAN)\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Training (normal only) - fixed to avoid double-backward graph issue\n",
    "# ------------------------------\n",
    "BS, EPOCHS = 256, 60\n",
    "loader = DataLoader(TensorDataset(torch.from_numpy(X_tr_n)), batch_size=BS, shuffle=True)\n",
    "\n",
    "def set_requires_grad(module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    loss_g_tot = loss_d_tot = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    for (xb,) in loader:\n",
    "        xb = xb.to(device)\n",
    "\n",
    "        # ==== (1) Update D ====\n",
    "        set_requires_grad(model.disc, True)   # bật grad cho D\n",
    "        opt_D.zero_grad()\n",
    "\n",
    "        # tạo x_hat để huấn luyện D, NGẮT gradient từ G\n",
    "        with torch.no_grad():\n",
    "            z_tmp  = model.enc1(xb)\n",
    "            x_hat_tmp = model.dec(z_tmp)\n",
    "\n",
    "        d_real, _ = model.disc(xb)\n",
    "        d_fake, _ = model.disc(x_hat_tmp.detach())\n",
    "\n",
    "        # hinge loss cho D\n",
    "        loss_d = torch.relu(1.0 - d_real).mean() + torch.relu(1.0 + d_fake).mean()\n",
    "        loss_d.backward()          # KHÔNG retain_graph\n",
    "        opt_D.step()\n",
    "\n",
    "        # ==== (2) Update G ====\n",
    "        set_requires_grad(model.disc, False)  # đóng băng D khi tối ưu G\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        # re-forward SAU khi D đã được update (graph mới hoàn toàn)\n",
    "        x_hat, z, z_hat, d_real2, d_fake2, f_real2, f_fake2 = model(xb)\n",
    "\n",
    "        # feature matching: không backprop vào D -> detach f_real2\n",
    "        loss_fm  = l1(f_real2.detach(), f_fake2)\n",
    "        loss_con = l1(x_hat, xb)                       # tái tạo L1 mean\n",
    "        loss_enc = torch.mean((z - z_hat) ** 2)        # latent consistency\n",
    "\n",
    "        loss_g = LAMBDA_FM * loss_fm + LAMBDA_CON * loss_con + LAMBDA_ENC * loss_enc\n",
    "        loss_g.backward()\n",
    "        opt_G.step()\n",
    "\n",
    "        # thống kê\n",
    "        loss_g_tot += loss_g.item()\n",
    "        loss_d_tot += loss_d.item()\n",
    "        steps += 1\n",
    "\n",
    "    if ep == 1 or ep % 5 == 0:\n",
    "        print(f\"[GANomaly] Epoch {ep:3d}/{EPOCHS} | L_G={loss_g_tot/steps:.4f} | L_D={loss_d_tot/steps:.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7) Scoring (chuẩn hóa 0..1 theo VAL)\n",
    "#     score = α*recon + β*latent + γ*feat\n",
    "# ------------------------------\n",
    "ALPHA, BETA, GAMMA = 0.7, 0.3, 0.0  # tabular: chủ yếu recon + latent\n",
    "\n",
    "@torch.no_grad()\n",
    "def anomaly_score(x_np: np.ndarray):\n",
    "    model.eval()\n",
    "    xt = torch.from_numpy(x_np).to(device)\n",
    "    x_hat, z, z_hat, d_real, d_fake, f_real, f_fake = model(xt)\n",
    "    recon  = torch.mean(torch.abs(x_hat - xt), dim=1)         # L1 mean\n",
    "    latent = torch.mean((z - z_hat)**2, dim=1)                # L2 mean\n",
    "    feat   = torch.mean(torch.abs(f_real - f_fake), dim=1)    # option\n",
    "    s = ALPHA*recon + BETA*latent + GAMMA*feat\n",
    "    return s.detach().cpu().numpy().astype(np.float64), \\\n",
    "           recon.cpu().numpy(), latent.cpu().numpy(), feat.cpu().numpy()\n",
    "\n",
    "# Scores trên VAL để min–max\n",
    "val_s_raw, _, _, _ = anomaly_score(X_val)\n",
    "smin, smax = float(np.min(val_s_raw)), float(np.max(val_s_raw)); eps=1e-12\n",
    "def to_norm(s): return (s - smin) / (smax - smin + eps)\n",
    "\n",
    "val_s = to_norm(val_s_raw)\n",
    "\n",
    "# ------------------------------\n",
    "# 8) Chọn ngưỡng (trên thang 0..1) & Đánh giá\n",
    "# ------------------------------\n",
    "def best_f1_threshold(y_true, scores, percentiles=np.linspace(50,99.5,200)):\n",
    "    ths = np.percentile(scores, percentiles)\n",
    "    best_f1, best_thr = -1.0, None\n",
    "    for t in ths:\n",
    "        yhat = (scores >= t).astype(int)\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y_true, yhat, labels=[0,1], average=None, zero_division=0)\n",
    "        if f1[1] > best_f1: best_f1, best_thr = float(f1[1]), float(t)\n",
    "    return best_thr, best_f1\n",
    "\n",
    "thr_norm, f1v = best_f1_threshold(y_val, val_s)\n",
    "print(f\"\\n[VAL balanced] Best F1(Class 1)={f1v:.3f} at thr(norm)={thr_norm:.6f}\")\n",
    "\n",
    "def evaluate(name, X, y, thr):\n",
    "    s_raw,_,_,_ = anomaly_score(X)\n",
    "    s = to_norm(s_raw)  # dùng thang chuẩn hóa 0..1\n",
    "    yhat = (s >= thr).astype(int)\n",
    "    print(f\"\\n===== {name} @thr(norm)={thr:.6f} =====\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y, yhat))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y, yhat, digits=4))\n",
    "    print(\"ROC AUC (scores):\", roc_auc_score(y, s))\n",
    "\n",
    "evaluate(\"TEST Balanced (200/200)\", X_tstb, y_tstb, thr_norm)\n",
    "evaluate(\"TEST Imbalanced (10000/200)\", X_tsti, y_tsti, thr_norm)\n",
    "\n",
    "# Sweep vài percentile để quan sát trade-off\n",
    "print(\"\\n>>> Quick sweep on percentile-based thresholds (norm-scale):\")\n",
    "for pctl in [60,70,80,85,90,92.5,95,97.5,99]:\n",
    "    t = float(np.percentile(val_s, pctl))\n",
    "    print(f\"\\n-- Try thr(norm)={t:.6f} (pctl={pctl}) on TEST Balanced --\")\n",
    "    evaluate(\"TEST Balanced (200/200)\", X_tstb, y_tstb, t)\n",
    "    print(f\"\\n-- Try thr(norm)={t:.6f} (pctl={pctl}) on TEST Imbalanced --\")\n",
    "    evaluate(\"TEST Imbalanced (10000/200)\", X_tsti, y_tsti, t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
