{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1a7d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Counts -> Normal: 9661 | Anomaly: 339\n",
      "[WARN] Không đủ anomaly theo yêu cầu. Sẽ dùng tối đa: TEST_A=339, TEST_B=339\n",
      "Train normal size: 8000\n",
      "TEST_A dist (200/400 mong muốn): {0: 200, 1: 339}\n",
      "TEST_B dist (200/800 mong muốn): {0: 200, 1: 339}\n",
      "Epoch   1/40 | D: 0.8702 | G: 1.5420\n",
      "Epoch   5/40 | D: 0.5726 | G: 2.1599\n",
      "Epoch  10/40 | D: 0.8472 | G: 2.6718\n",
      "Epoch  15/40 | D: 1.1864 | G: 1.0912\n",
      "Epoch  20/40 | D: 1.2860 | G: 0.8688\n",
      "Epoch  25/40 | D: 1.3260 | G: 0.7184\n",
      "Epoch  30/40 | D: 1.2951 | G: 0.7364\n",
      "Epoch  35/40 | D: 1.3655 | G: 0.7040\n",
      "Epoch  40/40 | D: 1.3360 | G: 0.7459\n",
      "\n",
      "[Train95th] thr=0.765946\n",
      "[TEST_A maxF1] thr=0.630570 | F1_1=0.2736\n",
      "[TEST_B maxF1] thr=0.714399 | F1_1=0.1806\n",
      "\n",
      "===== TEST_A (200 normal + 400 anomaly) - Train95th @thr=0.765946 =====\n",
      "Confusion Matrix:\n",
      " [[200   0]\n",
      " [313  26]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3899    1.0000    0.5610       200\n",
      "           1     1.0000    0.0767    0.1425       339\n",
      "\n",
      "    accuracy                         0.4193       539\n",
      "   macro avg     0.6949    0.5383    0.3517       539\n",
      "weighted avg     0.7736    0.4193    0.2978       539\n",
      "\n",
      "ROC AUC (scores): 0.22122418879056044\n",
      "\n",
      "===== TEST_A (200 normal + 400 anomaly) - BestF1 @thr=0.630570 =====\n",
      "Confusion Matrix:\n",
      " [[173  27]\n",
      " [281  58]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3811    0.8650    0.5291       200\n",
      "           1     0.6824    0.1711    0.2736       339\n",
      "\n",
      "    accuracy                         0.4286       539\n",
      "   macro avg     0.5317    0.5180    0.4013       539\n",
      "weighted avg     0.5706    0.4286    0.3684       539\n",
      "\n",
      "ROC AUC (scores): 0.22122418879056044\n",
      "\n",
      "===== TEST_B (200 normal + 800 anomaly) - Train95th @thr=0.765946 =====\n",
      "Confusion Matrix:\n",
      " [[183  17]\n",
      " [313  26]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3690    0.9150    0.5259       200\n",
      "           1     0.6047    0.0767    0.1361       339\n",
      "\n",
      "    accuracy                         0.3878       539\n",
      "   macro avg     0.4868    0.4958    0.3310       539\n",
      "weighted avg     0.5172    0.3878    0.2807       539\n",
      "\n",
      "ROC AUC (scores): 0.14113569321533925\n",
      "\n",
      "===== TEST_B (200 normal + 800 anomaly) - BestF1 @thr=0.714399 =====\n",
      "Confusion Matrix:\n",
      " [[136  64]\n",
      " [299  40]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3126    0.6800    0.4283       200\n",
      "           1     0.3846    0.1180    0.1806       339\n",
      "\n",
      "    accuracy                         0.3265       539\n",
      "   macro avg     0.3486    0.3990    0.3045       539\n",
      "weighted avg     0.3579    0.3265    0.2725       539\n",
      "\n",
      "ROC AUC (scores): 0.14113569321533925\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_fscore_support\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# =========================\n",
    "# 0) Reproducibility & device\n",
    "# =========================\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# =========================\n",
    "# 1) Load & preprocess\n",
    "# =========================\n",
    "csv_path = \"../../data/ai4i2020.csv\"   # đổi \"/mnt/data/ai4i2020.csv\" nếu cần\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "drop_cols = ['UDI', 'Product ID', 'Type', 'Machine failure', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF']\n",
    "features = df.drop(columns=drop_cols)\n",
    "labels = df['Machine failure'].astype(int).to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_all = scaler.fit_transform(features).astype(np.float32)\n",
    "\n",
    "# =========================\n",
    "# 2) Build sets (giống VAE: 200/400 và 200/800)\n",
    "# =========================\n",
    "normal_idx = np.where(labels == 0)[0]\n",
    "anom_idx   = np.where(labels == 1)[0]\n",
    "n_normal, n_anom = len(normal_idx), len(anom_idx)\n",
    "print(f\"Counts -> Normal: {n_normal} | Anomaly: {n_anom}\")\n",
    "\n",
    "assert n_normal >= 8400, \"Cần >= 8400 normal để 8000 train + 200 + 200 test.\"\n",
    "\n",
    "# Train: ONLY normal\n",
    "X_train = X_all[normal_idx[:8000]]\n",
    "\n",
    "# 2 test sets: 200 normal khác nhau để \"sạch\"\n",
    "testA_normal = X_all[normal_idx[8000:8200]]   # 200 normal\n",
    "testB_normal = X_all[normal_idx[8200:8400]]   # 200 normal\n",
    "\n",
    "# Yêu cầu anomaly\n",
    "reqA_anom, reqB_anom = 400, 800\n",
    "gotA_anom = min(reqA_anom, n_anom)\n",
    "gotB_anom = min(reqB_anom, n_anom)\n",
    "if gotA_anom < reqA_anom or gotB_anom < reqB_anom:\n",
    "    print(f\"[WARN] Không đủ anomaly theo yêu cầu. Sẽ dùng tối đa: \"\n",
    "          f\"TEST_A={gotA_anom}, TEST_B={gotB_anom}\")\n",
    "\n",
    "# Dùng cùng phân phối anomaly (lấy từ đầu danh sách)\n",
    "testA_anom = X_all[anom_idx[:gotA_anom]]\n",
    "testB_anom = X_all[anom_idx[:gotB_anom]]\n",
    "\n",
    "X_testA = np.vstack([testA_normal, testA_anom]).astype(np.float32)\n",
    "y_testA = np.hstack([np.zeros(testA_normal.shape[0], dtype=int),\n",
    "                     np.ones(testA_anom.shape[0], dtype=int)])\n",
    "\n",
    "X_testB = np.vstack([testB_normal, testB_anom]).astype(np.float32)\n",
    "y_testB = np.hstack([np.zeros(testB_normal.shape[0], dtype=int),\n",
    "                     np.ones(testB_anom.shape[0], dtype=int)])\n",
    "\n",
    "print(\"Train normal size:\", X_train.shape[0])\n",
    "print(\"TEST_A dist (200/400 mong muốn):\", {0: int((y_testA==0).sum()), 1: int((y_testA==1).sum())})\n",
    "print(\"TEST_B dist (200/800 mong muốn):\", {0: int((y_testB==0).sum()), 1: int((y_testB==1).sum())})\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train)\n",
    "X_testA_t = torch.from_numpy(X_testA)\n",
    "X_testB_t = torch.from_numpy(X_testB)\n",
    "\n",
    "# =========================\n",
    "# 3) Define GAN (MLP)\n",
    "# =========================\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 256), nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)  # z-score space\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "latent_dim = 16\n",
    "input_dim  = X_train.shape[1]\n",
    "G = Generator(latent_dim, input_dim).to(device)\n",
    "D = Discriminator(input_dim).to(device)\n",
    "\n",
    "opt_g = optim.Adam(G.parameters(), lr=1e-3)\n",
    "opt_d = optim.Adam(D.parameters(), lr=1e-3)\n",
    "bce   = nn.BCELoss()\n",
    "\n",
    "# =========================\n",
    "# 4) Train GAN (ONLY normal)\n",
    "# =========================\n",
    "batch_size = 128\n",
    "epochs     = 40   # giảm 10–20 để chạy nhanh nếu không có GPU\n",
    "train_loader = DataLoader(TensorDataset(X_train_t), batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    G.train(); D.train()\n",
    "    ep_d = ep_g = 0.0; steps = 0\n",
    "    for (real_batch,) in train_loader:\n",
    "        real_batch = real_batch.to(device)\n",
    "        bsz = real_batch.size(0)\n",
    "\n",
    "        # --- Train D ---\n",
    "        opt_d.zero_grad()\n",
    "        z = torch.randn(bsz, latent_dim, device=device)\n",
    "        fake_batch = G(z).detach()\n",
    "\n",
    "        real_labels = torch.ones(bsz, 1, device=device)\n",
    "        fake_labels = torch.zeros(bsz, 1, device=device)\n",
    "\n",
    "        d_real = D(real_batch)\n",
    "        d_fake = D(fake_batch)\n",
    "\n",
    "        d_loss = bce(d_real, real_labels) + bce(d_fake, fake_labels)\n",
    "        d_loss.backward()\n",
    "        opt_d.step()\n",
    "\n",
    "        # --- Train G ---\n",
    "        opt_g.zero_grad()\n",
    "        z = torch.randn(bsz, latent_dim, device=device)\n",
    "        gen_batch = G(z)\n",
    "        g_loss = bce(D(gen_batch), real_labels)  # trick D\n",
    "        g_loss.backward()\n",
    "        opt_g.step()\n",
    "\n",
    "        ep_d += d_loss.item(); ep_g += g_loss.item(); steps += 1\n",
    "\n",
    "    if epoch == 1 or epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch:3d}/{epochs} | D: {ep_d/steps:.4f} | G: {ep_g/steps:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# 5) Scoring & thresholds\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def disc_scores(x_np: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Anomaly score = 1 - D(x)  (cao hơn => bất thường hơn)\"\"\"\n",
    "    X_t = torch.from_numpy(x_np).to(device)\n",
    "    D.eval()\n",
    "    probs = D(X_t).cpu().numpy().reshape(-1)\n",
    "    return 1.0 - probs\n",
    "\n",
    "train_scores = disc_scores(X_train)\n",
    "testA_scores = disc_scores(X_testA)\n",
    "testB_scores = disc_scores(X_testB)\n",
    "\n",
    "# Ngưỡng theo train (không rò rỉ): 95th percentile\n",
    "thr_train95 = float(np.percentile(train_scores, 95.0))\n",
    "\n",
    "def best_f1_threshold(y_true, scores, percentiles=np.linspace(80, 99.9, 200)):\n",
    "    thrs = np.percentile(scores, percentiles)\n",
    "    best_f1, best_thr = -1.0, None\n",
    "    for t in thrs:\n",
    "        y_pred = (scores >= t).astype(int)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=[0,1], average=None, zero_division=0)\n",
    "        if f1[1] > best_f1:\n",
    "            best_f1, best_thr = f1[1], t\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "thrA_opt, f1A_opt = best_f1_threshold(y_testA, testA_scores)\n",
    "thrB_opt, f1B_opt = best_f1_threshold(y_testB, testB_scores)\n",
    "\n",
    "print(f\"\\n[Train95th] thr={thr_train95:.6f}\")\n",
    "print(f\"[TEST_A maxF1] thr={thrA_opt:.6f} | F1_1={f1A_opt:.4f}\")\n",
    "print(f\"[TEST_B maxF1] thr={thrB_opt:.6f} | F1_1={f1B_opt:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# 6) Evaluate\n",
    "# =========================\n",
    "def evaluate(name, scores, y_true, thr_use):\n",
    "    y_pred = (scores >= thr_use).astype(int)\n",
    "    print(f\"\\n===== {name} @thr={thr_use:.6f} =====\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"ROC AUC (scores):\", roc_auc_score(y_true, scores))\n",
    "\n",
    "evaluate(\"TEST_A (200 normal + 400 anomaly) - Train95th\", testA_scores, y_testA, thr_train95)\n",
    "evaluate(\"TEST_A (200 normal + 400 anomaly) - BestF1\",    testA_scores, y_testA, thrA_opt)\n",
    "\n",
    "evaluate(\"TEST_B (200 normal + 800 anomaly) - Train95th\", testB_scores, y_testB, thr_train95)\n",
    "evaluate(\"TEST_B (200 normal + 800 anomaly) - BestF1\",    testB_scores, y_testB, thrB_opt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
