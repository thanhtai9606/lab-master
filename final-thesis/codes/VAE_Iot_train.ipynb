{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dab8eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Counts -> Normal: 9661 | Anomaly: 339\n",
      "[WARN] Không đủ anomaly theo yêu cầu. Sẽ dùng tối đa: TEST_A=339 anomaly, TEST_B=339 anomaly.\n",
      "Train normal size: 8000\n",
      "TEST_A dist (200/400 mong muốn): {0: 200, 1: 339}\n",
      "TEST_B dist (200/800 mong muốn): {0: 200, 1: 339}\n",
      "Epoch   1/40 | loss=0.794748 | recon=0.678909 | kl=0.115839\n",
      "Epoch   5/40 | loss=0.516554 | recon=0.231281 | kl=0.285273\n",
      "Epoch  10/40 | loss=0.517975 | recon=0.229551 | kl=0.288425\n",
      "Epoch  15/40 | loss=0.511042 | recon=0.224404 | kl=0.286638\n",
      "Epoch  20/40 | loss=0.514568 | recon=0.225287 | kl=0.289281\n",
      "Epoch  25/40 | loss=0.511787 | recon=0.223783 | kl=0.288003\n",
      "Epoch  30/40 | loss=0.511829 | recon=0.224208 | kl=0.287621\n",
      "Epoch  35/40 | loss=0.513064 | recon=0.222136 | kl=0.290928\n",
      "Epoch  40/40 | loss=0.513160 | recon=0.224509 | kl=0.288651\n",
      "\n",
      "[Train95th] thr=0.554915\n",
      "[TEST_A maxF1] thr=0.442104 | F1_1=0.3535\n",
      "[TEST_B maxF1] thr=0.436837 | F1_1=0.4251\n",
      "\n",
      "===== TEST_A (200 normal + 400 anomaly) - Train95th @thr=0.554915 =====\n",
      "Confusion Matrix:\n",
      " [[186  14]\n",
      " [295  44]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3867    0.9300    0.5463       200\n",
      "           1     0.7586    0.1298    0.2217       339\n",
      "\n",
      "    accuracy                         0.4267       539\n",
      "   macro avg     0.5727    0.5299    0.3840       539\n",
      "weighted avg     0.6206    0.4267    0.3421       539\n",
      "\n",
      "ROC AUC (scores): 0.609188790560472\n",
      "\n",
      "===== TEST_A (200 normal + 400 anomaly) - BestF1 @thr=0.442104 =====\n",
      "Confusion Matrix:\n",
      " [[171  29]\n",
      " [260  79]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3968    0.8550    0.5420       200\n",
      "           1     0.7315    0.2330    0.3535       339\n",
      "\n",
      "    accuracy                         0.4638       539\n",
      "   macro avg     0.5641    0.5440    0.4477       539\n",
      "weighted avg     0.6073    0.4638    0.4234       539\n",
      "\n",
      "ROC AUC (scores): 0.609188790560472\n",
      "\n",
      "===== TEST_B (200 normal + 800 anomaly) - Train95th @thr=0.554915 =====\n",
      "Confusion Matrix:\n",
      " [[195   5]\n",
      " [286  53]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4054    0.9750    0.5727       200\n",
      "           1     0.9138    0.1563    0.2670       339\n",
      "\n",
      "    accuracy                         0.4601       539\n",
      "   macro avg     0.6596    0.5657    0.4198       539\n",
      "weighted avg     0.7252    0.4601    0.3804       539\n",
      "\n",
      "ROC AUC (scores): 0.6889085545722714\n",
      "\n",
      "===== TEST_B (200 normal + 800 anomaly) - BestF1 @thr=0.436837 =====\n",
      "Confusion Matrix:\n",
      " [[187  13]\n",
      " [244  95]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4339    0.9350    0.5927       200\n",
      "           1     0.8796    0.2802    0.4251       339\n",
      "\n",
      "    accuracy                         0.5232       539\n",
      "   macro avg     0.6568    0.6076    0.5089       539\n",
      "weighted avg     0.7142    0.5232    0.4873       539\n",
      "\n",
      "ROC AUC (scores): 0.6889085545722714\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_fscore_support\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# =========================\n",
    "# 0) Reproducibility & device\n",
    "# =========================\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# =========================\n",
    "# 1) Load & preprocess\n",
    "# =========================\n",
    "csv_path = \"../../data/ai4i2020.csv\"   # đổi thành \"/mnt/data/ai4i2020.csv\" nếu cần\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "drop_cols = ['UDI', 'Product ID', 'Type', 'Machine failure', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF']\n",
    "features = df.drop(columns=drop_cols)\n",
    "labels = df['Machine failure'].astype(int).to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_all = scaler.fit_transform(features).astype(np.float32)\n",
    "\n",
    "# =========================\n",
    "# 2) Build sets theo yêu cầu\n",
    "#    - Train: 8000 normal\n",
    "#    - TEST_A: 200 normal + 400 anomaly (giới hạn nếu thiếu)\n",
    "#    - TEST_B: 200 normal + 800 anomaly (giới hạn nếu thiếu)\n",
    "# =========================\n",
    "normal_idx = np.where(labels == 0)[0]\n",
    "anom_idx   = np.where(labels == 1)[0]\n",
    "n_normal = len(normal_idx)\n",
    "n_anom   = len(anom_idx)\n",
    "print(f\"Counts -> Normal: {n_normal} | Anomaly: {n_anom}\")\n",
    "\n",
    "assert n_normal >= 8400, \"Cần >= 8400 normal để lấy 8000 train + 200 + 200 test.\"\n",
    "\n",
    "# Train: ONLY normal\n",
    "X_train = X_all[normal_idx[:8000]]\n",
    "\n",
    "# Lấy normal cho 2 test set (không trùng nhau để sạch)\n",
    "testA_normal = X_all[normal_idx[8000:8200]]   # 200 normal\n",
    "testB_normal = X_all[normal_idx[8200:8400]]   # 200 normal\n",
    "\n",
    "# Yêu cầu anomaly\n",
    "reqA_anom = 400\n",
    "reqB_anom = 800\n",
    "\n",
    "# Giới hạn theo thực tế\n",
    "gotA_anom = min(reqA_anom, n_anom)\n",
    "gotB_anom = min(reqB_anom, n_anom)\n",
    "\n",
    "if gotA_anom < reqA_anom or gotB_anom < reqB_anom:\n",
    "    print(f\"[WARN] Không đủ anomaly theo yêu cầu. Sẽ dùng tối đa: \"\n",
    "          f\"TEST_A={gotA_anom} anomaly, TEST_B={gotB_anom} anomaly.\")\n",
    "\n",
    "# Dùng cùng tập anomaly đầu cho 2 test set (đảm bảo dùng chung phân phối)\n",
    "testA_anom = X_all[anom_idx[:gotA_anom]]\n",
    "testB_anom = X_all[anom_idx[:gotB_anom]]\n",
    "\n",
    "X_testA = np.vstack([testA_normal, testA_anom]).astype(np.float32)\n",
    "y_testA = np.hstack([np.zeros(testA_normal.shape[0], dtype=int),\n",
    "                     np.ones(testA_anom.shape[0], dtype=int)])\n",
    "\n",
    "X_testB = np.vstack([testB_normal, testB_anom]).astype(np.float32)\n",
    "y_testB = np.hstack([np.zeros(testB_normal.shape[0], dtype=int),\n",
    "                     np.ones(testB_anom.shape[0], dtype=int)])\n",
    "\n",
    "print(\"Train normal size:\", X_train.shape[0])\n",
    "print(\"TEST_A dist (200/400 mong muốn):\", {0: int((y_testA==0).sum()), 1: int((y_testA==1).sum())})\n",
    "print(\"TEST_B dist (200/800 mong muốn):\", {0: int((y_testB==0).sum()), 1: int((y_testB==1).sum())})\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train)\n",
    "X_testA_t = torch.from_numpy(X_testA)\n",
    "X_testB_t = torch.from_numpy(X_testB)\n",
    "\n",
    "# =========================\n",
    "# 3) VAE definition\n",
    "# =========================\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=8, hidden=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU()\n",
    "        )\n",
    "        self.enc_mu    = nn.Linear(hidden, latent_dim)\n",
    "        self.enc_logvar= nn.Linear(hidden, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, input_dim)\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h   = self.encoder(x)\n",
    "        mu  = self.enc_mu(h)\n",
    "        lv  = self.enc_logvar(h)\n",
    "        z   = self.reparameterize(mu, lv)\n",
    "        xhat= self.decoder(z)\n",
    "        return xhat, mu, lv, z\n",
    "\n",
    "def vae_loss_fn(x, xhat, mu, logvar, recon=\"mse\", beta=1.0):\n",
    "    if recon == \"mse\":\n",
    "        recon_loss = nn.functional.mse_loss(xhat, x, reduction=\"mean\")\n",
    "    else:\n",
    "        recon_loss = nn.functional.l1_loss(xhat, x, reduction=\"mean\")\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl, recon_loss, kl\n",
    "\n",
    "# =========================\n",
    "# 4) Train VAE on normal only\n",
    "# =========================\n",
    "input_dim = X_train.shape[1]\n",
    "vae = VAE(input_dim, latent_dim=8, hidden=128).to(device)\n",
    "opt = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 40  # giảm 10–20 nếu cần chạy nhanh\n",
    "train_loader = DataLoader(TensorDataset(X_train_t), batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    vae.train()\n",
    "    total = recon_total = kl_total = 0.0\n",
    "    steps = 0\n",
    "    for (xb,) in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        opt.zero_grad()\n",
    "        xhat, mu, lv, z = vae(xb)\n",
    "        loss, rl, kl = vae_loss_fn(xb, xhat, mu, lv, recon=\"mse\", beta=1.0)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item(); recon_total += rl.item(); kl_total += kl.item(); steps += 1\n",
    "    if ep == 1 or ep % 5 == 0:\n",
    "        print(f\"Epoch {ep:3d}/{epochs} | loss={total/steps:.6f} | recon={recon_total/steps:.6f} | kl={kl_total/steps:.6f}\")\n",
    "\n",
    "# =========================\n",
    "# 5) Scoring: reconstruction error\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def recon_error(x_np: np.ndarray, model: VAE, agg=\"mse\"):\n",
    "    model.eval()\n",
    "    xt = torch.from_numpy(x_np).to(device)\n",
    "    xhat, _, _, _ = model(xt)\n",
    "    if agg == \"mse\":\n",
    "        err = torch.mean((xhat - xt)**2, dim=1)\n",
    "    else:\n",
    "        err = torch.mean(torch.abs(xhat - xt), dim=1)\n",
    "    return err.detach().cpu().numpy()\n",
    "\n",
    "train_err = recon_error(X_train, vae, agg=\"mse\")\n",
    "testA_err = recon_error(X_testA, vae, agg=\"mse\")\n",
    "testB_err = recon_error(X_testB, vae, agg=\"mse\")\n",
    "\n",
    "# =========================\n",
    "# 6) Thresholds\n",
    "# =========================\n",
    "thr_train95 = float(np.percentile(train_err, 95.0))\n",
    "\n",
    "def best_f1_threshold(y_true, scores, percentiles=np.linspace(80, 99.9, 200)):\n",
    "    thrs = np.percentile(scores, percentiles)\n",
    "    best_f1, best_thr = -1.0, None\n",
    "    for t in thrs:\n",
    "        y_pred = (scores >= t).astype(int)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=[0,1], average=None, zero_division=0)\n",
    "        if f1[1] > best_f1:\n",
    "            best_f1, best_thr = f1[1], t\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "thrA_opt, f1A_opt = best_f1_threshold(y_testA, testA_err)\n",
    "thrB_opt, f1B_opt = best_f1_threshold(y_testB, testB_err)\n",
    "\n",
    "print(f\"\\n[Train95th] thr={thr_train95:.6f}\")\n",
    "print(f\"[TEST_A maxF1] thr={thrA_opt:.6f} | F1_1={f1A_opt:.4f}\")\n",
    "print(f\"[TEST_B maxF1] thr={thrB_opt:.6f} | F1_1={f1B_opt:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# 7) Evaluate\n",
    "# =========================\n",
    "def evaluate(name, scores, y_true, thr_use):\n",
    "    y_pred = (scores >= thr_use).astype(int)\n",
    "    print(f\"\\n===== {name} @thr={thr_use:.6f} =====\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"ROC AUC (scores):\", roc_auc_score(y_true, scores))\n",
    "\n",
    "evaluate(\"TEST_A (200 normal + 400 anomaly) - Train95th\", testA_err, y_testA, thr_train95)\n",
    "evaluate(\"TEST_A (200 normal + 400 anomaly) - BestF1\",    testA_err, y_testA, thrA_opt)\n",
    "\n",
    "evaluate(\"TEST_B (200 normal + 800 anomaly) - Train95th\", testB_err, y_testB, thr_train95)\n",
    "evaluate(\"TEST_B (200 normal + 800 anomaly) - BestF1\",    testB_err, y_testB, thrB_opt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
