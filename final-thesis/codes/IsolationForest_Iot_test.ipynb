{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91d2c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts -> Normal: 9661 | Anomaly: 339\n",
      "[WARN] Không đủ anomaly theo yêu cầu. Sẽ dùng tối đa: TEST_A=339, TEST_B=339\n",
      "Train normal size: 8000\n",
      "TEST_A dist (200/400 mong muốn): {0: 200, 1: 339}\n",
      "TEST_B dist (200/800 mong muốn): {0: 200, 1: 339}\n",
      "\n",
      "[Train95th] thr=0.574772\n",
      "[TEST_A maxF1] thr=0.566733 | F1_1=0.4653\n",
      "[TEST_B maxF1] thr=0.566917 | F1_1=0.4653\n",
      "\n",
      "===== TEST_A (200 normal + 400 anomaly) - Train95th @thr=0.574772 =====\n",
      "Confusion Matrix:\n",
      " [[196   4]\n",
      " [244  95]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4455    0.9800    0.6125       200\n",
      "           1     0.9596    0.2802    0.4338       339\n",
      "\n",
      "    accuracy                         0.5399       539\n",
      "   macro avg     0.7025    0.6301    0.5231       539\n",
      "weighted avg     0.7688    0.5399    0.5001       539\n",
      "\n",
      "ROC AUC (scores): 0.8741150442477876\n",
      "\n",
      "===== TEST_A (200 normal + 400 anomaly) - BestF1 @thr=0.566733 =====\n",
      "Confusion Matrix:\n",
      " [[196   4]\n",
      " [235 104]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4548    0.9800    0.6212       200\n",
      "           1     0.9630    0.3068    0.4653       339\n",
      "\n",
      "    accuracy                         0.5566       539\n",
      "   macro avg     0.7089    0.6434    0.5433       539\n",
      "weighted avg     0.7744    0.5566    0.5232       539\n",
      "\n",
      "ROC AUC (scores): 0.8741150442477876\n",
      "\n",
      "===== TEST_B (200 normal + 800 anomaly) - Train95th @thr=0.574772 =====\n",
      "Confusion Matrix:\n",
      " [[198   2]\n",
      " [244  95]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4480    0.9900    0.6168       200\n",
      "           1     0.9794    0.2802    0.4358       339\n",
      "\n",
      "    accuracy                         0.5436       539\n",
      "   macro avg     0.7137    0.6351    0.5263       539\n",
      "weighted avg     0.7822    0.5436    0.5030       539\n",
      "\n",
      "ROC AUC (scores): 0.7870058997050149\n",
      "\n",
      "===== TEST_B (200 normal + 800 anomaly) - BestF1 @thr=0.566917 =====\n",
      "Confusion Matrix:\n",
      " [[196   4]\n",
      " [235 104]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4548    0.9800    0.6212       200\n",
      "           1     0.9630    0.3068    0.4653       339\n",
      "\n",
      "    accuracy                         0.5566       539\n",
      "   macro avg     0.7089    0.6434    0.5433       539\n",
      "weighted avg     0.7744    0.5566    0.5232       539\n",
      "\n",
      "ROC AUC (scores): 0.7870058997050149\n"
     ]
    }
   ],
   "source": [
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "# =========================\n",
    "# 0) Reproducibility\n",
    "# =========================\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# =========================\n",
    "# 1) Load & preprocess\n",
    "# =========================\n",
    "csv_path = \"../../data/ai4i2020.csv\"  # đổi \"/mnt/data/ai4i2020.csv\" nếu cần\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "drop_cols = ['UDI', 'Product ID', 'Type', 'Machine failure', 'TWF', 'HDF', 'PWF', 'OSF', 'RNF']\n",
    "features = df.drop(columns=drop_cols)\n",
    "labels = df['Machine failure'].astype(int).to_numpy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_all = scaler.fit_transform(features).astype(np.float32)\n",
    "\n",
    "# =========================\n",
    "# 2) Split (giống VAE/GAN): Train 8000 normal; TEST_A: 200/400; TEST_B: 200/800\n",
    "# =========================\n",
    "normal_idx = np.where(labels == 0)[0]\n",
    "anom_idx   = np.where(labels == 1)[0]\n",
    "n_normal, n_anom = len(normal_idx), len(anom_idx)\n",
    "print(f\"Counts -> Normal: {n_normal} | Anomaly: {n_anom}\")\n",
    "assert n_normal >= 8400, \"Cần >= 8400 normal để 8000 train + 200 + 200 test.\"\n",
    "\n",
    "X_train = X_all[normal_idx[:8000]]\n",
    "\n",
    "testA_normal = X_all[normal_idx[8000:8200]]  # 200 normal\n",
    "testB_normal = X_all[normal_idx[8200:8400]]  # 200 normal\n",
    "\n",
    "reqA_anom, reqB_anom = 400, 800\n",
    "gotA_anom = min(reqA_anom, n_anom)\n",
    "gotB_anom = min(reqB_anom, n_anom)\n",
    "if gotA_anom < reqA_anom or gotB_anom < reqB_anom:\n",
    "    print(f\"[WARN] Không đủ anomaly theo yêu cầu. Sẽ dùng tối đa: TEST_A={gotA_anom}, TEST_B={gotB_anom}\")\n",
    "\n",
    "testA_anom = X_all[anom_idx[:gotA_anom]]\n",
    "testB_anom = X_all[anom_idx[:gotB_anom]]\n",
    "\n",
    "X_testA = np.vstack([testA_normal, testA_anom]).astype(np.float32)\n",
    "y_testA = np.hstack([np.zeros(testA_normal.shape[0], dtype=int),\n",
    "                     np.ones(testA_anom.shape[0], dtype=int)])\n",
    "\n",
    "X_testB = np.vstack([testB_normal, testB_anom]).astype(np.float32)\n",
    "y_testB = np.hstack([np.zeros(testB_normal.shape[0], dtype=int),\n",
    "                     np.ones(testB_anom.shape[0], dtype=int)])\n",
    "\n",
    "print(\"Train normal size:\", X_train.shape[0])\n",
    "print(\"TEST_A dist (200/400 mong muốn):\", {0: int((y_testA==0).sum()), 1: int((y_testA==1).sum())})\n",
    "print(\"TEST_B dist (200/800 mong muốn):\", {0: int((y_testB==0).sum()), 1: int((y_testB==1).sum())})\n",
    "\n",
    "# =========================\n",
    "# 3) Fit Isolation Forest (only normal)\n",
    "# =========================\n",
    "# Không dùng predict() để phụ thuộc contamination; tự tính score và tự đặt ngưỡng.\n",
    "clf = IsolationForest(\n",
    "    n_estimators=300,\n",
    "    max_samples='auto',\n",
    "    contamination='auto',   # không ảnh hưởng vì ta tự threshold\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    bootstrap=False\n",
    ")\n",
    "clf.fit(X_train)\n",
    "\n",
    "# =========================\n",
    "# 4) Anomaly scores (cao hơn => bất thường hơn)\n",
    "# =========================\n",
    "# score_samples: higher => more normal, nên đảo dấu\n",
    "def anomaly_score(model, X):\n",
    "    return -model.score_samples(X)\n",
    "\n",
    "train_scores = anomaly_score(clf, X_train)\n",
    "testA_scores = anomaly_score(clf, X_testA)\n",
    "testB_scores = anomaly_score(clf, X_testB)\n",
    "\n",
    "# =========================\n",
    "# 5) Thresholds\n",
    "# =========================\n",
    "thr_train95 = float(np.percentile(train_scores, 95.0))\n",
    "\n",
    "def best_f1_threshold(y_true, scores, percentiles=np.linspace(80, 99.9, 200)):\n",
    "    thrs = np.percentile(scores, percentiles)\n",
    "    best_f1, best_thr = -1.0, None\n",
    "    for t in thrs:\n",
    "        y_pred = (scores >= t).astype(int)  # 1 = anomaly\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=[0,1], average=None, zero_division=0)\n",
    "        if f1[1] > best_f1:\n",
    "            best_f1, best_thr = f1[1], t\n",
    "    return float(best_thr), float(best_f1)\n",
    "\n",
    "thrA_opt, f1A_opt = best_f1_threshold(y_testA, testA_scores)\n",
    "thrB_opt, f1B_opt = best_f1_threshold(y_testB, testB_scores)\n",
    "\n",
    "print(f\"\\n[Train95th] thr={thr_train95:.6f}\")\n",
    "print(f\"[TEST_A maxF1] thr={thrA_opt:.6f} | F1_1={f1A_opt:.4f}\")\n",
    "print(f\"[TEST_B maxF1] thr={thrB_opt:.6f} | F1_1={f1B_opt:.4f}\")\n",
    "\n",
    "# =========================\n",
    "# 6) Evaluate\n",
    "# =========================\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "def evaluate(name, scores, y_true, thr_use):\n",
    "    y_pred = (scores >= thr_use).astype(int)\n",
    "    print(f\"\\n===== {name} @thr={thr_use:.6f} =====\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n",
    "    print(\"ROC AUC (scores):\", roc_auc_score(y_true, scores))\n",
    "\n",
    "evaluate(\"TEST_A (200 normal + 400 anomaly) - Train95th\", testA_scores, y_testA, thr_train95)\n",
    "evaluate(\"TEST_A (200 normal + 400 anomaly) - BestF1\",    testA_scores, y_testA, thrA_opt)\n",
    "\n",
    "evaluate(\"TEST_B (200 normal + 800 anomaly) - Train95th\", testB_scores, y_testB, thr_train95)\n",
    "evaluate(\"TEST_B (200 normal + 800 anomaly) - BestF1\",    testB_scores, y_testB, thrB_opt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
