{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68afa4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (170883, 29) Val: (56962, 29) Test: (56962, 29)\n",
      "\n",
      "=== Isolation Forest (Test Real) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9987    0.9970    0.9979     56887\n",
      "           1     0.0000    0.0000    0.0000        75\n",
      "\n",
      "    accuracy                         0.9957     56962\n",
      "   macro avg     0.4993    0.4985    0.4989     56962\n",
      "weighted avg     0.9974    0.9957    0.9965     56962\n",
      "\n",
      "[[56719   168]\n",
      " [   75     0]]\n",
      "ROC-AUC: 0.9492516743720006\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     83\u001b[39m vae = keras.Model(inputs, recons)\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m recon_loss = tf.reduce_mean(\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43msquare\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecons\u001b[49m\u001b[43m)\u001b[49m, axis=-\u001b[32m1\u001b[39m)\n\u001b[32m     87\u001b[39m kl_loss = -\u001b[32m0.5\u001b[39m * tf.reduce_sum(\u001b[32m1\u001b[39m + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-\u001b[32m1\u001b[39m)\n\u001b[32m     88\u001b[39m vae.add_loss(K.mean(recon_loss + kl_loss))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[39m, in \u001b[36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     87\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m   bound_arguments = signature.bind(*args, **kwargs)\n\u001b[32m     90\u001b[39m   bound_arguments.apply_defaults()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py:12067\u001b[39m, in \u001b[36msquare\u001b[39m\u001b[34m(x, name)\u001b[39m\n\u001b[32m  12065\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m  12066\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m> \u001b[39m\u001b[32m12067\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msquare_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12068\u001b[39m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  12069\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _core._SymbolicException:\n\u001b[32m  12070\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/tensorflow/python/ops/gen_math_ops.py:12108\u001b[39m, in \u001b[36msquare_eager_fallback\u001b[39m\u001b[34m(x, name, ctx)\u001b[39m\n\u001b[32m  12107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msquare_eager_fallback\u001b[39m(x: Annotated[Any, TV_Square_T], name, ctx) -> Annotated[Any, TV_Square_T]:\n\u001b[32m> \u001b[39m\u001b[32m12108\u001b[39m   _attr_T, (x,) = \u001b[43m_execute\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs_to_matching_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplex64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplex128\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  12109\u001b[39m   _inputs_flat = [x]\n\u001b[32m  12110\u001b[39m   _attrs = (\u001b[33m\"\u001b[39m\u001b[33mT\u001b[39m\u001b[33m\"\u001b[39m, _attr_T)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:251\u001b[39m, in \u001b[36margs_to_matching_eager\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# First see if we can get a valid dtype with the default conversion\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# and see if it matches an allowed dtypes. Some ops like ConcatV2 may\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# not list allowed dtypes, in which case we should skip this.\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m allowed_dtypes:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m   tensor = \u001b[43mtensor_conversion_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m   \u001b[38;5;66;03m# If we did not match an allowed dtype, try again with the default\u001b[39;00m\n\u001b[32m    253\u001b[39m   \u001b[38;5;66;03m# dtype. This could be because we have an empty tensor and thus we\u001b[39;00m\n\u001b[32m    254\u001b[39m   \u001b[38;5;66;03m# picked the wrong type.\u001b[39;00m\n\u001b[32m    255\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m tensor.dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_dtypes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:209\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[39m\n\u001b[32m    207\u001b[39m overload = \u001b[38;5;28mgetattr\u001b[39m(value, \u001b[33m\"\u001b[39m\u001b[33m__tf_tensor__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m overload \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moverload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#  pylint: disable=not-callable\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m base_type, conversion_func \u001b[38;5;129;01min\u001b[39;00m get(\u001b[38;5;28mtype\u001b[39m(value)):\n\u001b[32m    212\u001b[39m   \u001b[38;5;66;03m# If dtype is None but preferred_dtype is not None, we try to\u001b[39;00m\n\u001b[32m    213\u001b[39m   \u001b[38;5;66;03m# cast to preferred_dtype first.\u001b[39;00m\n\u001b[32m    214\u001b[39m   ret = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/keras/src/backend/common/keras_tensor.py:194\u001b[39m, in \u001b[36mKerasTensor.__tf_tensor__\u001b[39m\u001b[34m(self, dtype, name)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    195\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mused when constructing Keras Functional models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand `keras.ops`). \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    204\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    210\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    211\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    212\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    213\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    214\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "K = keras.backend\n",
    "\n",
    "# =========================\n",
    "# 1. Load & Split Data\n",
    "# =========================\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "features = df.drop(columns=[\"Time\", \"Class\"])\n",
    "labels   = df[\"Class\"].values\n",
    "\n",
    "# Train = 60%, Val = 20%, Test = 20%\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(features, labels, test_size=0.2, shuffle=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, shuffle=False)\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "# Balanced validation & test sets\n",
    "fraud_val  = X_val[y_val == 1]\n",
    "normal_val = X_val[y_val == 0].sample(n=len(fraud_val), random_state=42)\n",
    "X_val_bal  = pd.concat([fraud_val, normal_val])\n",
    "y_val_bal  = np.array([1]*len(fraud_val) + [0]*len(normal_val))\n",
    "\n",
    "fraud_test  = X_test[y_test == 1]\n",
    "normal_test = X_test[y_test == 0].sample(n=len(fraud_test), random_state=42)\n",
    "X_test_bal  = pd.concat([fraud_test, normal_test])\n",
    "y_test_bal  = np.array([1]*len(fraud_test) + [0]*len(normal_test))\n",
    "\n",
    "# Convert numpy for DL\n",
    "X_train_np = X_train.to_numpy().astype(\"float32\")\n",
    "X_val_bal_np  = X_val_bal.to_numpy().astype(\"float32\")\n",
    "X_test_np     = X_test.to_numpy().astype(\"float32\")\n",
    "X_test_bal_np = X_test_bal.to_numpy().astype(\"float32\")\n",
    "\n",
    "# =========================\n",
    "# 2. Isolation Forest\n",
    "# =========================\n",
    "iso = IsolationForest(n_estimators=200, random_state=42, contamination=\"auto\")\n",
    "iso.fit(X_train[y_train == 0])  # chỉ train trên normal\n",
    "\n",
    "val_scores  = -iso.score_samples(X_val_bal)\n",
    "test_scores = -iso.score_samples(X_test)\n",
    "thr = np.percentile(val_scores, 95)  # hoặc chọn theo F1\n",
    "y_pred_test = (test_scores >= thr).astype(int)\n",
    "\n",
    "print(\"\\n=== Isolation Forest (Test Real) ===\")\n",
    "print(classification_report(y_test, y_pred_test, digits=4))\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, test_scores))\n",
    "\n",
    "# =========================\n",
    "# 3. Variational Autoencoder (VAE)\n",
    "# =========================\n",
    "input_dim = X_train_np.shape[1]\n",
    "latent_dim = 8\n",
    "\n",
    "# Encoder\n",
    "inputs = keras.Input(shape=(input_dim,))\n",
    "h = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_var = layers.Dense(latent_dim)(h)\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    eps = K.random_normal(shape=K.shape(z_mean))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * eps\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "encoder = keras.Model(inputs, [z_mean, z_log_var, z])\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(64, activation=\"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(input_dim, activation=\"linear\")(x)\n",
    "decoder = keras.Model(latent_inputs, outputs)\n",
    "\n",
    "# VAE model\n",
    "recons = decoder(encoder(inputs)[2])\n",
    "vae = keras.Model(inputs, recons)\n",
    "\n",
    "# Loss\n",
    "recon_loss = tf.reduce_mean(tf.square(inputs - recons), axis=-1)\n",
    "kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "vae.add_loss(K.mean(recon_loss + kl_loss))\n",
    "vae.compile(optimizer=\"adam\")\n",
    "vae.fit(X_train_np[y_train == 0], X_train_np[y_train == 0], epochs=20, batch_size=256, verbose=2)\n",
    "\n",
    "# Anomaly detection\n",
    "recon_val = vae.predict(X_val_bal_np, verbose=0)\n",
    "mse_val = np.mean(np.square(X_val_bal_np - recon_val), axis=1)\n",
    "thr_vae = np.percentile(mse_val, 95)\n",
    "\n",
    "recon_test = vae.predict(X_test_np, verbose=0)\n",
    "mse_test = np.mean(np.square(X_test_np - recon_test), axis=1)\n",
    "y_pred_vae = (mse_test > thr_vae).astype(int)\n",
    "\n",
    "print(\"\\n=== VAE (Test Real) ===\")\n",
    "print(classification_report(y_test, y_pred_vae, digits=4))\n",
    "print(confusion_matrix(y_test, y_pred_vae))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, mse_test))\n",
    "\n",
    "# =========================\n",
    "# 4. GAN (Simple AnoGAN style)\n",
    "# =========================\n",
    "latent_dim = 16\n",
    "generator = keras.Sequential([\n",
    "    layers.Dense(32, activation=\"relu\", input_dim=latent_dim),\n",
    "    layers.Dense(input_dim, activation=\"linear\")\n",
    "])\n",
    "discriminator = keras.Sequential([\n",
    "    layers.Dense(32, activation=\"relu\", input_dim=input_dim),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "discriminator.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "discriminator.trainable = False\n",
    "z = keras.Input(shape=(latent_dim,))\n",
    "gen_out = generator(z)\n",
    "gan_out = discriminator(gen_out)\n",
    "gan = keras.Model(z, gan_out)\n",
    "gan.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "\n",
    "X_train_norm = X_train_np[y_train == 0]\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.randint(0, X_train_norm.shape[0], batch_size)\n",
    "    real = X_train_norm[idx]\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    fake = generator.predict(noise, verbose=0)\n",
    "\n",
    "    d_loss_real = discriminator.train_on_batch(real, np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(fake, np.zeros((batch_size, 1)))\n",
    "    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, D Loss: {(d_loss_real+d_loss_fake)/2}, G Loss: {g_loss}\")\n",
    "\n",
    "# Anomaly score = 1 - discriminator confidence\n",
    "scores = 1 - discriminator.predict(X_test_np, verbose=0).reshape(-1)\n",
    "thr_gan = np.percentile(scores, 95)\n",
    "y_pred_gan = (scores > thr_gan).astype(int)\n",
    "\n",
    "print(\"\\n=== GAN (Test Real) ===\")\n",
    "print(classification_report(y_test, y_pred_gan, digits=4))\n",
    "print(confusion_matrix(y_test, y_pred_gan))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
