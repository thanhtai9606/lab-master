{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Credit Card Fraud - Time-split + Balanced VAL\n",
    "# Models: Isolation Forest (CPU), VAE (DL), GAN (DL)\n",
    "# - Train: ONLY normal (Class=0)\n",
    "# - VAL (balanced): chọn ngưỡng bằng F1(Class 1) hoặc precision>=X\n",
    "# - TEST: test_real (mặc định, imbalanced) + test_balanced (tùy chọn)\n",
    "# ==========================================\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    precision_recall_fscore_support, precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "# ---------- Reproducibility ----------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# ---------- Config ----------\n",
    "CSV_PATH = \"creditcard.csv\"  # đổi path nếu cần\n",
    "DROP_COLS = [\"Class\", \"Amount\", \"Time\"]\n",
    "\n",
    "# ---------- Load ----------\n",
    "df_raw = pd.read_csv(CSV_PATH)\n",
    "assert {\"Time\",\"Amount\",\"Class\"}.issubset(df_raw.columns), \"Thiếu cột Time/Amount/Class trong creditcard.csv\"\n",
    "\n",
    "# ---------- Time-split ----------\n",
    "df = df_raw.sort_values(\"Time\").reset_index(drop=True)\n",
    "n = len(df)\n",
    "cut1 = int(0.6*n)\n",
    "cut2 = int(0.8*n)\n",
    "\n",
    "train_part = df.iloc[:cut1].copy()\n",
    "val_part   = df.iloc[cut1:cut2].copy()\n",
    "test_real  = df.iloc[cut2:].copy()\n",
    "\n",
    "# ---------- Build VAL balanced ----------\n",
    "fraud_val  = val_part[val_part.Class == 1]\n",
    "if len(fraud_val) == 0:\n",
    "    raise ValueError(\"VAL không có fraud; hãy đổi tỷ lệ split hoặc chọn vùng thời gian khác.\")\n",
    "normal_val = val_part[val_part.Class == 0].sample(n=len(fraud_val), random_state=SEED, replace=False)\n",
    "val_balanced = pd.concat([normal_val, fraud_val]).sample(frac=1, random_state=SEED)\n",
    "\n",
    "# ---------- Train-only-normal (cho IF/VAE/GAN) ----------\n",
    "train_normal = train_part[train_part.Class == 0].copy()\n",
    "if len(train_normal) == 0:\n",
    "    raise ValueError(\"TRAIN không có normal records.\")\n",
    "\n",
    "# ---------- Scaler: fit trên TRAIN NORMAL để tránh rò rỉ ----------\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(train_normal.drop(columns=[\"Class\",\"Time\",\"Amount\"]))\n",
    "train_normal_scaled = pd.DataFrame(X_train_norm, columns=[c for c in train_normal.columns if c not in [\"Class\",\"Time\",\"Amount\"]])\n",
    "train_normal_scaled[\"Class\"] = 0\n",
    "\n",
    "def scale_dataframe(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = scaler.transform(df_in.drop(columns=[\"Class\",\"Time\",\"Amount\"]))\n",
    "    out = pd.DataFrame(X, columns=[c for c in df_in.columns if c not in [\"Class\",\"Time\",\"Amount\"]])\n",
    "    out[\"Class\"] = df_in[\"Class\"].to_numpy()\n",
    "    return out\n",
    "\n",
    "val_bal_scaled = scale_dataframe(val_balanced)\n",
    "test_real_scaled = scale_dataframe(test_real)\n",
    "\n",
    "# Optional: xây test_balanced để đối chiếu\n",
    "fraud_test  = test_real[test_real.Class==1]\n",
    "normal_test = test_real[test_real.Class==0]\n",
    "take = min(len(fraud_test), len(normal_test))\n",
    "test_balanced = pd.concat([\n",
    "    normal_test.sample(n=take, random_state=SEED, replace=False),\n",
    "    fraud_test.sample(n=take, random_state=SEED, replace=False)\n",
    "]).sample(frac=1, random_state=SEED)\n",
    "test_bal_scaled = scale_dataframe(test_balanced)\n",
    "\n",
    "# ======================================================\n",
    "# Common utils: thresholding & evaluation\n",
    "# ======================================================\n",
    "def eval_at_threshold(y_true, scores, thr):\n",
    "    y_pred = (scores >= thr).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=[0,1], average=None, zero_division=0\n",
    "    )\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    return dict(threshold=float(thr), TN=int(tn), FP=int(fp), FN=int(fn), TP=int(tp),\n",
    "                precision_1=float(prec[1]), recall_1=float(rec[1]), f1_1=float(f1[1]),\n",
    "                accuracy=float(acc))\n",
    "\n",
    "def best_f1_threshold(y_true, scores, percentiles=np.linspace(50, 99.5, 200)):\n",
    "    thrs = np.percentile(scores, percentiles)\n",
    "    rows = [eval_at_threshold(y_true, scores, t) for t in thrs]\n",
    "    df = pd.DataFrame(rows)\n",
    "    return float(df.iloc[df[\"f1_1\"].idxmax()][\"threshold\"]), df\n",
    "\n",
    "def threshold_for_precision(y_true, scores, target_p=0.50):\n",
    "    p, r, thr = precision_recall_curve(y_true, scores)\n",
    "    idx = np.where(p >= target_p)[0]\n",
    "    if len(idx) == 0 or len(thr) == 0:\n",
    "        return None\n",
    "    i = idx[0]\n",
    "    thr_sel = thr[i-1] if i > 0 else thr[0]\n",
    "    return float(thr_sel), float(p[i]), float(r[i])\n",
    "\n",
    "def print_full_report(title, y_true, scores, thr):\n",
    "    y_pred = (scores >= thr).astype(int)\n",
    "    print(f\"\\n===== {title} @ thr={thr:.6f} =====\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=3))\n",
    "    print(f\"ROC AUC (scores): {roc_auc_score(y_true, scores):.3f}\")\n",
    "    print(f\"PR  AUC (AP):    {average_precision_score(y_true, scores):.3f}\")\n",
    "\n",
    "# ======================================================\n",
    "# 1) Isolation Forest\n",
    "# ======================================================\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def run_isoforest(train_df_scaled, val_df_scaled, test_df_scaled, param):\n",
    "    X_train = train_df_scaled.drop(columns=[\"Class\"])\n",
    "    X_val   = val_df_scaled.drop(columns=[\"Class\"])\n",
    "    X_test  = test_df_scaled.drop(columns=[\"Class\"])\n",
    "    y_val   = val_df_scaled[\"Class\"].to_numpy()\n",
    "    y_test  = test_df_scaled[\"Class\"].to_numpy()\n",
    "\n",
    "    iso = IsolationForest(\n",
    "        n_estimators=param.get(\"n_estimators\", 300),\n",
    "        max_samples=param.get(\"max_samples\", \"auto\"),\n",
    "        contamination=\"auto\",\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    iso.fit(X_train)\n",
    "    val_scores  = -iso.score_samples(X_val)\n",
    "    test_scores = -iso.score_samples(X_test)\n",
    "\n",
    "    thr_f1, _ = best_f1_threshold(y_val, val_scores)\n",
    "    print_full_report(\"IF - TEST (BestF1 on VAL_balanced)\", y_test, test_scores, thr_f1)\n",
    "\n",
    "    res = threshold_for_precision(y_val, val_scores, target_p=0.50)\n",
    "    if res:\n",
    "        thr_p50, p50, r50 = res\n",
    "        print(f\"[IF][VAL_bal] thr for Precision≥50%: {thr_p50:.6f} (P={p50:.3f}, R={r50:.3f})\")\n",
    "        print_full_report(\"IF - TEST (Precision≥50% on VAL_balanced)\", y_test, test_scores, thr_p50)\n",
    "    return test_scores, thr_f1\n",
    "\n",
    "# ======================================================\n",
    "# 2) VAE (train on normal)\n",
    "# ======================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=8, hidden=128):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU()\n",
    "        )\n",
    "        self.enc_mu    = nn.Linear(hidden, latent_dim)\n",
    "        self.enc_logvar= nn.Linear(hidden, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, input_dim)\n",
    "        )\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar); eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, lv = self.enc_mu(h), self.enc_logvar(h)\n",
    "        z = self.reparameterize(mu, lv)\n",
    "        xhat = self.decoder(z)\n",
    "        return xhat, mu, lv, z\n",
    "\n",
    "def vae_loss_fn(x, xhat, mu, logvar, recon=\"mse\", beta=1.0):\n",
    "    if recon == \"mse\":\n",
    "        recon_loss = nn.functional.mse_loss(xhat, x, reduction=\"mean\")\n",
    "    else:\n",
    "        recon_loss = nn.functional.l1_loss(xhat, x, reduction=\"mean\")\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl, recon_loss, kl\n",
    "\n",
    "def run_vae(train_df_scaled, val_df_scaled, test_df_scaled, epochs=30, batch_size=256, latent_dim=8, hidden=128):\n",
    "    Xtr = train_df_scaled.drop(columns=[\"Class\"]).to_numpy().astype(np.float32)\n",
    "    Xva = val_df_scaled.drop(columns=[\"Class\"]).to_numpy().astype(np.float32)\n",
    "    Xte = test_df_scaled.drop(columns=[\"Class\"]).to_numpy().astype(np.float32)\n",
    "    y_val = val_df_scaled[\"Class\"].to_numpy()\n",
    "    y_test= test_df_scaled[\"Class\"].to_numpy()\n",
    "\n",
    "    vae = VAE(input_dim=Xtr.shape[1], latent_dim=latent_dim, hidden=hidden).to(device)\n",
    "    opt = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "    loader = DataLoader(TensorDataset(torch.from_numpy(Xtr)), batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        vae.train(); tot=recon=kl=0.0; steps=0\n",
    "        for (xb,) in loader:\n",
    "            xb = xb.to(device); opt.zero_grad()\n",
    "            xhat, mu, lv, z = vae(xb)\n",
    "            loss, rl, klv = vae_loss_fn(xb, xhat, mu, lv, recon=\"mse\", beta=1.0)\n",
    "            loss.backward(); opt.step()\n",
    "            tot+=loss.item(); recon+=rl.item(); kl+=klv.item(); steps+=1\n",
    "        if ep==1 or ep%5==0: print(f\"[VAE] Epoch {ep:2d}/{epochs} | loss={tot/steps:.5f} | recon={recon/steps:.5f} | kl={kl/steps:.5f}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def recon_err(X):\n",
    "        xt = torch.from_numpy(X).to(device)\n",
    "        xhat, _, _, _ = vae(xt)\n",
    "        err = torch.mean((xhat-xt)**2, dim=1).cpu().numpy()\n",
    "        return err\n",
    "\n",
    "    val_scores  = recon_err(Xva)\n",
    "    test_scores = recon_err(Xte)\n",
    "\n",
    "    thr_f1, _ = best_f1_threshold(y_val, val_scores)\n",
    "    print_full_report(\"VAE - TEST (BestF1 on VAL_balanced)\", y_test, test_scores, thr_f1)\n",
    "\n",
    "    res = threshold_for_precision(y_val, val_scores, target_p=0.50)\n",
    "    if res:\n",
    "        thr_p50, p50, r50 = res\n",
    "        print(f\"[VAE][VAL_bal] thr for Precision≥50%: {thr_p50:.6f} (P={p50:.3f}, R={r50:.3f})\")\n",
    "        print_full_report(\"VAE - TEST (Precision≥50% on VAL_balanced)\", y_test, test_scores, thr_p50)\n",
    "    return test_scores, thr_f1\n",
    "\n",
    "# ======================================================\n",
    "# 3) GAN (Discriminator scoring: 1 - D(x))\n",
    "# ======================================================\n",
    "class Gen(nn.Module):\n",
    "    def __init__(self, latent_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim,128), nn.ReLU(),\n",
    "            nn.Linear(128,256), nn.ReLU(),\n",
    "            nn.Linear(256,out_dim)\n",
    "        )\n",
    "    def forward(self, z): return self.net(z)\n",
    "\n",
    "class Disc(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim,256), nn.ReLU(),\n",
    "            nn.Linear(256,128), nn.ReLU(),\n",
    "            nn.Linear(128,1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "def run_gan(train_df_scaled, val_df_scaled, test_df_scaled, epochs=30, batch_size=256, latent_dim=16):\n",
    "    Xtr = train_df_scaled.drop(columns=[\"Class\"]).to_numpy().astype(np.float32)\n",
    "    Xva = val_df_scaled.drop(columns=[\"Class\"]).to_numpy().astype(np.float32)\n",
    "    Xte = test_df_scaled.drop(columns=[\"Class\"]).to_numpy().astype(np.float32)\n",
    "    y_val = val_df_scaled[\"Class\"].to_numpy()\n",
    "    y_test= test_df_scaled[\"Class\"].to_numpy()\n",
    "\n",
    "    G = Gen(latent_dim, Xtr.shape[1]).to(device)\n",
    "    D = Disc(Xtr.shape[1]).to(device)\n",
    "    opt_g = optim.Adam(G.parameters(), lr=1e-3)\n",
    "    opt_d = optim.Adam(D.parameters(), lr=1e-3)\n",
    "    bce = nn.BCELoss()\n",
    "\n",
    "    loader = DataLoader(TensorDataset(torch.from_numpy(Xtr)), batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        G.train(); D.train(); dsum=gsum=0.0; steps=0\n",
    "        for (xb,) in loader:\n",
    "            xb = xb.to(device); bsz = xb.size(0)\n",
    "            # D\n",
    "            opt_d.zero_grad()\n",
    "            z = torch.randn(bsz, latent_dim, device=device)\n",
    "            fake = G(z).detach()\n",
    "            real_labels = torch.ones(bsz,1,device=device)\n",
    "            fake_labels = torch.zeros(bsz,1,device=device)\n",
    "            d_loss = bce(D(xb), real_labels) + bce(D(fake), fake_labels)\n",
    "            d_loss.backward(); opt_d.step()\n",
    "            # G\n",
    "            opt_g.zero_grad()\n",
    "            z = torch.randn(bsz, latent_dim, device=device)\n",
    "            gen = G(z)\n",
    "            g_loss = bce(D(gen), real_labels)\n",
    "            g_loss.backward(); opt_g.step()\n",
    "            dsum+=d_loss.item(); gsum+=g_loss.item(); steps+=1\n",
    "        if ep==1 or ep%5==0: print(f\"[GAN] Epoch {ep:2d}/{epochs} | D={dsum/steps:.5f} | G={gsum/steps:.5f}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def d_scores(X):\n",
    "        Xt = torch.from_numpy(X).to(device)\n",
    "        D.eval()\n",
    "        prob = D(Xt).cpu().numpy().reshape(-1)\n",
    "        return 1.0 - prob  # cao => bất thường\n",
    "\n",
    "    val_scores  = d_scores(Xva)\n",
    "    test_scores = d_scores(Xte)\n",
    "\n",
    "    thr_f1, _ = best_f1_threshold(y_val, val_scores)\n",
    "    print_full_report(\"GAN - TEST (BestF1 on VAL_balanced)\", y_test, test_scores, thr_f1)\n",
    "\n",
    "    res = threshold_for_precision(y_val, val_scores, target_p=0.50)\n",
    "    if res:\n",
    "        thr_p50, p50, r50 = res\n",
    "        print(f\"[GAN][VAL_bal] thr for Precision≥50%: {thr_p50:.6f} (P={p50:.3f}, R={r50:.3f})\")\n",
    "        print_full_report(\"GAN - TEST (Precision≥50% on VAL_balanced)\", y_test, test_scores, thr_p50)\n",
    "    return test_scores, thr_f1\n",
    "\n",
    "# ======================================================\n",
    "# ----------------- RUN EXPERIMENTS --------------------\n",
    "# Chọn test set: test_real_scaled (imbalanced) hoặc test_bal_scaled (balanced)\n",
    "TEST_SET_NAME = \"test_real\"  # đổi \"test_balanced\" nếu muốn\n",
    "test_scaled = test_real_scaled if TEST_SET_NAME==\"test_real\" else test_bal_scaled\n",
    "print(f\"\\n>>> Using TEST set: {TEST_SET_NAME} | size={len(test_scaled)} | positives={int((test_scaled.Class==1).sum())}\")\n",
    "\n",
    "# Isolation Forest\n",
    "print(\"\\n================ Isolation Forest ================\")\n",
    "_ifscores, _ifthr = run_isoforest(train_normal_scaled, val_bal_scaled, test_scaled,\n",
    "                                  param={\"n_estimators\":400, \"max_samples\":512})\n",
    "\n",
    "# VAE\n",
    "print(\"\\n====================== VAE ======================\")\n",
    "_vaescores, _vaethr = run_vae(train_normal_scaled, val_bal_scaled, test_scaled,\n",
    "                              epochs=30, batch_size=512, latent_dim=8, hidden=128)\n",
    "\n",
    "# GAN\n",
    "print(\"\\n====================== GAN ======================\")\n",
    "_ganscores, _ganthr = run_gan(train_normal_scaled, val_bal_scaled, test_scaled,\n",
    "                              epochs=30, batch_size=512, latent_dim=16)\n",
    "\n",
    "print(\"\\n>>> DONE. So sánh ROC/PR, F1 theo các ngưỡng ở trên để chọn mô hình.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
