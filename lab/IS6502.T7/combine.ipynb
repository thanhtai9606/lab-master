{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.stats import uniform, randint\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " | UDI   | Product ID   | Type   | Air temperature [K]   | Process temperature [K]   | Rotational speed [rpm]   | Torque [Nm]   | Tool wear [min]   | Machine failure   | TWF   | HDF   | PWF   | OSF   | RNF   |\n",
      "|:------|:-------------|:-------|:----------------------|:--------------------------|:-------------------------|:--------------|:------------------|:------------------|:------|:------|:------|:------|:------|\n",
      "| 1     | M14860       | M      | 298.1                 | 308.6                     | 1551                     | 42.8          | 0                 | 0                 | 0     | 0     | 0     | 0     | 0     |\n",
      "| 2     | L47181       | L      | 298.2                 | 308.7                     | 1408                     | 46.3          | 3                 | 0                 | 0     | 0     | 0     | 0     | 0     |\n",
      "| 3     | L47182       | L      | 298.1                 | 308.5                     | 1498                     | 49.4          | 5                 | 0                 | 0     | 0     | 0     | 0     | 0     |\n",
      "| 4     | L47183       | L      | 298.2                 | 308.6                     | 1433                     | 39.5          | 7                 | 0                 | 0     | 0     | 0     | 0     | 0     |\n",
      "| 5     | L47184       | L      | 298.2                 | 308.7                     | 1408                     | 40            | 9                 | 0                 | 0     | 0     | 0     | 0     | 0     |\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = 'ai4i2020.csv'  # Update the path to your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "print(\"Original Data:\\n\", data.head().to_markdown(index=False, numalign='left', stralign='left'))  # Display first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and target variables\n",
    "X = data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]', 'Tool wear [min]']]\n",
    "y = data[['TWF', 'HDF', 'PWF', 'OSF', 'RNF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Data Shapes: X: (8000, 5) y: (8000, 5)\n",
      "Test Data Shapes: X: (2000, 5) y: (2000, 5)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"\\nTrain Data Shapes: X:\", X_train.shape, \"y:\", y_train.shape)\n",
    "print(\"Test Data Shapes: X:\", X_test.shape, \"y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of Scaled X_train (First 5 Rows):\n",
      " |    | Air temperature [K]   | Process temperature [K]   | Rotational speed [rpm]   | Torque [Nm]   | Tool wear [min]   |\n",
      "|:---|:----------------------|:--------------------------|:-------------------------|:--------------|:------------------|\n",
      "| 0  | -0.854066             | -0.609589                 | 0.427634                 | -0.892696     | 1.37504           |\n",
      "| 1  | -0.904014             | -1.08053                  | -0.834945                | 1.38219       | 0.45762           |\n",
      "| 2  | -0.904014             | -1.48419                  | -0.0596769               | -0.892696     | 1.35922           |\n",
      "| 3  | 0.444571              | 0.534121                  | 0.333495                 | -0.702288     | -1.59866          |\n",
      "| 4  | 0.694309              | 0.33229                   | 0.178441                 | -0.612094     | 1.58066           |\n"
     ]
    }
   ],
   "source": [
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nExample of Scaled X_train (First 5 Rows):\\n\", pd.DataFrame(X_train_scaled, columns=X.columns).head().to_markdown(numalign='left', stralign='left'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resampled Train Data Shapes: X: (79324, 5) y: (79324, 5)\n",
      "Example of Resampled X_train (First 5 Rows):\n",
      " | Air temperature [K]   | Process temperature [K]   | Rotational speed [rpm]   | Torque [Nm]   | Tool wear [min]   |\n",
      "|:----------------------|:--------------------------|:-------------------------|:--------------|:------------------|\n",
      "| -0.735487             | 0.000984106               | -0.10875                 | -0.229793     | -1.04547          |\n",
      "| -1.68162              | -1.55424                  | -0.656051                | 1.2623        | 1.13828           |\n",
      "| 1.56525               | 1.17545                   | -0.648754                | 0.439461      | -0.503101         |\n",
      "| 0.276051              | 0.806866                  | -0.0394249               | -0.280988     | -1.48793          |\n",
      "| 0.94394               | 0.199082                  | -0.553888                | 0.551887      | -0.631556         |\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTETomek separately for X and y\n",
    "smotetomek = SMOTETomek(random_state=42)\n",
    "X_train_resampled_list = []\n",
    "y_train_resampled = pd.DataFrame()\n",
    "for column in y_train.columns:\n",
    "    X_res, y_res = smotetomek.fit_resample(X_train, y_train[column])\n",
    "    X_train_resampled_list.append(pd.DataFrame(X_res, columns=X.columns))\n",
    "    y_train_resampled = pd.concat([y_train_resampled, pd.DataFrame(y_res, columns=[column])], axis=0)\n",
    "\n",
    "# Concatenate and shuffle resampled data\n",
    "X_train_resampled = pd.concat(X_train_resampled_list, axis=0)\n",
    "X_train_resampled, y_train_resampled = shuffle(X_train_resampled, y_train_resampled, random_state=42)\n",
    "\n",
    "# Scale the resampled features\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "print(\"\\nResampled Train Data Shapes: X:\", X_train_resampled.shape, \"y:\", y_train_resampled.shape)\n",
    "\n",
    "# Convert back to DataFrame for display\n",
    "X_train_resampled_df = pd.DataFrame(X_train_resampled, columns=X.columns)\n",
    "print(\"Example of Resampled X_train (First 5 Rows):\\n\", X_train_resampled_df.head().to_markdown(index=False, numalign='left', stralign='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values and re-clip to ensure 0/1 values in y_train_resampled\n",
    "y_train_resampled = y_train_resampled.fillna(0)  # Or fill with another appropriate value\n",
    "for column in y_train_resampled.columns:\n",
    "    y_train_resampled[column] = np.clip(y_train_resampled[column], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all resampled data have the same number of samples\n",
    "min_samples = min(len(x) for x in X_train_resampled_list)\n",
    "X_train_resampled = pd.concat([x.iloc[:min_samples].reset_index(drop=True) for x in X_train_resampled_list], axis=0)\n",
    "y_train_resampled = y_train_resampled.iloc[:min_samples].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features (after resampling)\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Reshape the data for CNN-LSTM model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_resampled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(X_train_resampled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X_train_resampled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m X_test_reshaped \u001b[38;5;241m=\u001b[39m X_test_scaled\u001b[38;5;241m.\u001b[39mreshape(X_test_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X_test_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# Reshape the data for CNN-LSTM model\n",
    "X_train_reshaped = X_train_resampled.values.reshape(X_train_resampled.shape[0], X_train_resampled.shape[1], 1)\n",
    "X_test_reshaped = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78950, 5, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_train_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train_reshaped\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43my_train_encoded\u001b[49m\u001b[38;5;241m.\u001b[39mshape) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_train_reshaped.shape)\n",
    "print(y_train_encoded.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN-LSTM model\n",
    "def create_cnn_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=True))  \n",
    "    model.add(Dropout(0.2))  \n",
    "    model.add(LSTM(50, activation='relu'))  \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\Downloads\\Deep Learning\\lab-master\\lab\\venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">23,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">102</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │        \u001b[38;5;34m23,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m102\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,814</span> (171.15 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43,814\u001b[0m (171.15 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,686</span> (170.65 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43,686\u001b[0m (170.65 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 78950\n'y' sizes: 15790\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m cnn_lstm_model \u001b[38;5;241m=\u001b[39m create_cnn_lstm_model(input_shape, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Slice y_train_encoded to match the current target variable\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mcnn_lstm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_encoded\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Store the trained model\u001b[39;00m\n\u001b[0;32m     18\u001b[0m cnn_lstm_models[target] \u001b[38;5;241m=\u001b[39m cnn_lstm_model\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\Downloads\\Deep Learning\\lab-master\\lab\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\Downloads\\Deep Learning\\lab-master\\lab\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\data_adapter_utils.py:114\u001b[0m, in \u001b[0;36mcheck_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    110\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mflatten(single_data)\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    113\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m sizes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 78950\n'y' sizes: 15790\n"
     ]
    }
   ],
   "source": [
    "# Train CNN-LSTM model for each target variable\n",
    "cnn_lstm_models = {}\n",
    "cnn_lstm_predictions = np.zeros((X_test_reshaped.shape[0], len(y_train.columns)))\n",
    "\n",
    "for i, target in enumerate(y_train.columns):\n",
    "    # Convert target variable to categorical\n",
    "    y_train_encoded = to_categorical(y_train_resampled[target], num_classes=2)\n",
    "    y_test_encoded = to_categorical(y_test[target], num_classes=2)\n",
    "    \n",
    "    # Create and train the model for the current target variable\n",
    "    input_shape = (X_train_reshaped.shape[1], 1)\n",
    "    cnn_lstm_model = create_cnn_lstm_model(input_shape, num_classes=2)\n",
    "\n",
    "    # Slice y_train_encoded to match the current target variable\n",
    "    cnn_lstm_model.fit(X_train_reshaped, y_train_encoded[:, i], epochs=10, batch_size=32, verbose=1)  \n",
    "\n",
    "    # Store the trained model\n",
    "    cnn_lstm_models[target] = cnn_lstm_model\n",
    "\n",
    "    # Get predictions for the current target variable\n",
    "    cnn_lstm_predictions[:, i] = np.argmax(cnn_lstm_model.predict(X_test_reshaped), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TWF'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m cnn_lstm_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((X_test_reshaped\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mcolumns)))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[1;32m----> 4\u001b[0m     cnn_lstm_predictions[:, i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mcnn_lstm_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test_reshaped), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Define base models with hyperparameter grids\u001b[39;00m\n\u001b[0;32m      7\u001b[0m param_dist \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestimator__rf__n_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m),\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestimator__rf__max_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestimator__gb__learning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: uniform(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     14\u001b[0m }\n",
      "\u001b[1;31mKeyError\u001b[0m: 'TWF'"
     ]
    }
   ],
   "source": [
    "# Get predictions from the CNN-LSTM models\n",
    "cnn_lstm_predictions = np.zeros((X_test_reshaped.shape[0], len(y_train.columns)))\n",
    "for i, target in enumerate(y_train.columns):\n",
    "    cnn_lstm_predictions[:, i] = np.argmax(cnn_lstm_models[target].predict(X_test_reshaped), axis=1)\n",
    "\n",
    "# Define base models with hyperparameter grids\n",
    "param_dist = {\n",
    "    'estimator__rf__n_estimators': randint(50, 100),\n",
    "    'estimator__rf__max_depth': [None, 10, 20],\n",
    "    'estimator__lr__C': uniform(0.1, 10),\n",
    "    'estimator__svm__C': uniform(0.1, 10),\n",
    "    'estimator__gb__n_estimators': randint(50, 100),\n",
    "    'estimator__gb__learning_rate': uniform(0.01, 0.2)\n",
    "}\n",
    "\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "    ('svm', SVC(kernel='linear', probability=True, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Create a Voting Classifier\n",
    "voting_clf = VotingClassifier(estimators=base_models, voting='soft')\n",
    "\n",
    "# Wrap the voting classifier in a MultiOutputClassifier\n",
    "multi_voting_clf = MultiOutputClassifier(voting_clf)\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = RandomizedSearchCV(estimator=multi_voting_clf, param_distributions=param_dist, n_iter=20, cv=3, n_jobs=-1, random_state=42, verbose=1)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Extract best parameters for each model\n",
    "best_rf_params = {key.split('__')[-1]: value for key, value in best_params.items() if 'rf' in key}\n",
    "best_lr_params = {key.split('__')[-1]: value for key, value in best_params.items() if 'lr' in key}\n",
    "best_svm_params = {key.split('__')[-1]: value for key, value in best_params.items() if 'svm' in key}\n",
    "best_gb_params = {key.split('__')[-1]: value for key, value in best_params.items() if 'gb' in key}\n",
    "\n",
    "# Define the base models with the best parameters\n",
    "rf_best = RandomForestClassifier(random_state=42, **best_rf_params)\n",
    "lr_best = LogisticRegression(random_state=42, max_iter=1000, **best_lr_params)\n",
    "svm_best = SVC(kernel='linear', probability=True, random_state=42, **best_svm_params)\n",
    "gb_best = GradientBoostingClassifier(random_state=42, **best_gb_params)\n",
    "\n",
    "# Create a new Voting Classifier with the best models\n",
    "voting_clf_best = VotingClassifier(estimators=[\n",
    "    ('rf', rf_best),\n",
    "    ('lr', lr_best),\n",
    "    ('svm', svm_best),\n",
    "    ('gb', gb_best)\n",
    "], voting='soft')\n",
    "\n",
    "# Train the Voting Classifier\n",
    "multi_voting_clf_best = MultiOutputClassifier(voting_clf_best)\n",
    "multi_voting_clf_best.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get predictions from the Voting Classifier\n",
    "voting_predictions = multi_voting_clf_best.predict(X_test_scaled)\n",
    "\n",
    "# Combine predictions using majority voting or averaging\n",
    "final_predictions = np.zeros_like(cnn_lstm_predictions)\n",
    "for i in range(cnn_lstm_predictions.shape[0]):\n",
    "    for j in range(cnn_lstm_predictions.shape[1]):\n",
    "        combined_prediction = [cnn_lstm_predictions[i, j], voting_predictions[i, j]]\n",
    "        final_predictions[i, j] = np.argmax(np.bincount(combined_prediction))\n",
    "\n",
    "# Evaluate the combined model\n",
    "combined_classification_reports = {}\n",
    "combined_confusion_matrices = {}\n",
    "for i, mode in enumerate(['TWF', 'HDF', 'PWF', 'OSF', 'RNF']):\n",
    "    combined_classification_reports[mode] = classification_report(y_test[mode], final_predictions[:, i])\n",
    "    combined_confusion_matrices[mode] = confusion_matrix(y_test[mode], final_predictions[:, i])\n",
    "\n",
    "# Display classification reports and confusion matrices for the combined model\n",
    "for mode in ['TWF', 'HDF', 'PWF', 'OSF', 'RNF']:\n",
    "    print(f\"Classification Report for {mode}:\\n{combined_classification_reports[mode]}\")\n",
    "    print(f\"Confusion Matrix for {mode}:\\n{combined_confusion_matrices[mode]}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
